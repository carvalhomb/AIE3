{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJXW_DgiSebM"
   },
   "source": [
    "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
    "\n",
    "In the following notebook we'll complete the following tasks:\n",
    "\n",
    "- ü§ù Breakout Room #1:\n",
    "  1. Install required libraries\n",
    "  2. Set Environment Variables\n",
    "  3. Creating our Tool Belt\n",
    "  4. Creating Our State\n",
    "  5. Creating and Compiling A Graph!\n",
    "  \n",
    "- ü§ù Breakout Room #2:\n",
    "  - Part 1: LangSmith Evaluator:\n",
    "    1. Creating an Evaluation Dataset\n",
    "    2. Adding Evaluators\n",
    "  - Part 2:\n",
    "    3. Adding Helpfulness Check and \"Loop\" Limits\n",
    "    4. LangGraph for the \"Patterns\" of GenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djQ3nRAgoF67"
   },
   "source": [
    "# ü§ù Breakout Room #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7pQDUhUnIo8"
   },
   "source": [
    "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
    "\n",
    "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
    "\n",
    "### Why Cycles?\n",
    "\n",
    "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
    "\n",
    "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effetively allowing us to recreate appliation flowcharts in code in an almost 1-to-1 fashion.\n",
    "\n",
    "### Why LangGraph?\n",
    "\n",
    "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_fLDElOVoop"
   },
   "source": [
    "## Task 1:  Dependencies\n",
    "\n",
    "We'll first install all our required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KaVwN269EttM",
    "outputId": "3b97db0d-d119-4b43-b964-47291c7dba1c"
   },
   "outputs": [],
   "source": [
    "!pip install -qU langchain langchain_openai langchain-community langgraph arxiv duckduckgo_search==5.3.1b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wujPjGJuoPwg"
   },
   "source": [
    "## Task 2: Environment Variables\n",
    "\n",
    "We'll want to set both our OpenAI API key and our LangSmith environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jdh8CoVWHRvs",
    "outputId": "761167a9-b570-421b-eb9c-8be3dc813f47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os\n",
    "# import getpass\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
    "\n",
    "import os\n",
    "#import getpass\n",
    "\n",
    "#os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")\n",
    "\n",
    "# Dotenv library to load configuration from .env file\n",
    "import dotenv\n",
    "dotenvfile = dotenv.find_dotenv()\n",
    "PROJECT_ROOT = os.path.dirname(dotenvfile)  # Use dotenv to find root of the project\n",
    "\n",
    "# Load configuration from .env file\n",
    "dotenv.load_dotenv(dotenvfile)\n",
    "\n",
    "#print(os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nv0glIDyHmRt",
    "outputId": "b0237b19-ada7-4836-edc2-228e694a5ecb"
   },
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE3 - LangGraph - {uuid4().hex[0:8]}\"\n",
    "#os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")\n",
    "#print(os.environ[\"LANGCHAIN_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBRyQmEAVzua"
   },
   "source": [
    "## Task 3: Creating our Tool Belt\n",
    "\n",
    "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
    "\n",
    "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
    "\n",
    "We'll leverage:\n",
    "\n",
    "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
    "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2k6n_Dob2F46"
   },
   "source": [
    "####üèóÔ∏è Activity #1:\n",
    "\n",
    "Please add the tools to use into our toolbelt.\n",
    "\n",
    "> NOTE: Each tool in our toolbelt should be a method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lAxaSvlfIeOg"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
    "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
    "\n",
    "tool_belt = [\n",
    "    DuckDuckGoSearchRun(),\n",
    "    ArxivQueryRun()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FdOjEslXdRR"
   },
   "source": [
    "### Actioning with Tools\n",
    "\n",
    "Now that we've created our tool belt - we need to create a process that will let us leverage them when we need them.\n",
    "\n",
    "We'll use the built-in [`ToolExecutor`](https://github.com/langchain-ai/langgraph/blob/fab950acfbf5fea46c9313dca34ee2ae01f1728b/libs/langgraph/langgraph/prebuilt/tool_executor.py#L50) to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cFr1m80-JZsD"
   },
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolExecutor\n",
    "\n",
    "tool_executor = ToolExecutor(tool_belt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VI-C669ZYVI5"
   },
   "source": [
    "### Model\n",
    "\n",
    "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
    "\n",
    "- OpenAI's GPT-3.5 and GPT-4\n",
    "- Anthropic's Claude\n",
    "- Google's Gemini\n",
    "\n",
    "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QkNS8rNZJs4z"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ugkj3GzuZpQv"
   },
   "source": [
    "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4OdMqFafZ_0V"
   },
   "outputs": [],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "functions = [convert_to_openai_function(t) for t in tool_belt]\n",
    "model = model.bind_functions(functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERzuGo6W18Lr"
   },
   "source": [
    "#### ‚ùì Question #1:\n",
    "\n",
    "How does the model determine which tool to use?\n",
    "\n",
    "---\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "It will use the tool's description to decide when it is appropriate to call it. By looking at those tools' source code, I see that the descriptions of those community plugins we imported are:\n",
    "\n",
    "- DuckDuckGo: \"A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.\"\n",
    "- ArXiV: \"A wrapper around Arxiv.org. Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.\"\n",
    "- \n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_296Ub96Z_H8"
   },
   "source": [
    "## Task 4: Putting the State in Stateful\n",
    "\n",
    "Earlier we used this phrasing:\n",
    "\n",
    "`coordinated multi-actor and stateful applications`\n",
    "\n",
    "So what does that \"stateful\" mean?\n",
    "\n",
    "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
    "\n",
    "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
    "\n",
    "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
    "\n",
    "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
    "\n",
    "1. We initialize our state object:\n",
    "  - `{\"messages\" : []}`\n",
    "2. Our user submits a query to our application.\n",
    "  - New State: `HumanMessage(#1)`\n",
    "  - `{\"messages\" : [HumanMessage(#1)}`\n",
    "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
    "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
    "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
    "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mxL9b_NZKUdL"
   },
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "  messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWsMhfO9grLu"
   },
   "source": [
    "## Task 5: It's Graphing Time!\n",
    "\n",
    "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
    "\n",
    "Let's take a second to refresh ourselves about what a graph is in this context.\n",
    "\n",
    "Graphs, also called networks in some circles, are a collection of connected objects.\n",
    "\n",
    "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
    "\n",
    "Let's look at a simple graph.\n",
    "\n",
    "![image](https://i.imgur.com/2NFLnIc.png)\n",
    "\n",
    "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
    "\n",
    "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
    "\n",
    "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
    "\n",
    "Let's create some nodes and expand on our diagram.\n",
    "\n",
    "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "91flJWtZLUrl"
   },
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolInvocation\n",
    "import json\n",
    "from langchain_core.messages import FunctionMessage\n",
    "\n",
    "def call_model(state):\n",
    "  messages = state[\"messages\"]\n",
    "  response = model.invoke(messages)\n",
    "  return {\"messages\" : [response]}\n",
    "\n",
    "def call_tool(state):\n",
    "  last_message = state[\"messages\"][-1]\n",
    "\n",
    "  action = ToolInvocation(\n",
    "      tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
    "      tool_input=json.loads(\n",
    "          last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "      )\n",
    "  )\n",
    "\n",
    "  response = tool_executor.invoke(action)\n",
    "\n",
    "  function_message = FunctionMessage(content=str(response), name=action.tool)\n",
    "\n",
    "  return {\"messages\" : [function_message]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bwR7MgWj3Wg"
   },
   "source": [
    "Now we have two total nodes. We have:\n",
    "\n",
    "- `call_model` is a node that will...well...call the model\n",
    "- `call_tool` is a node which will call a tool\n",
    "\n",
    "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_vF4_lgtmQNo"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", call_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8CjRlbVmRpW"
   },
   "source": [
    "Let's look at what we have so far:\n",
    "\n",
    "![image](https://i.imgur.com/md7inqG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaXHpPeSnOWC"
   },
   "source": [
    "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YGCbaYqRnmiw"
   },
   "outputs": [],
   "source": [
    "workflow.set_entry_point(\"agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUsfGoSpoF9U"
   },
   "source": [
    "![image](https://i.imgur.com/wNixpJe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Q_pQgHmoW0M"
   },
   "source": [
    "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
    "\n",
    "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
    "\n",
    "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
    "\n",
    "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
    "\n",
    "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1BZgb81VQf9o"
   },
   "outputs": [],
   "source": [
    "def should_continue(state):\n",
    "  last_message = state[\"messages\"][-1]\n",
    "\n",
    "  if \"function_call\" not in last_message.additional_kwargs:\n",
    "    return \"end\"\n",
    "\n",
    "  return \"continue\"\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\" : \"action\",\n",
    "        \"end\" : END\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Cvhcf4jp0Ce"
   },
   "source": [
    "Let's visualize what this looks like.\n",
    "\n",
    "![image](https://i.imgur.com/8ZNwKI5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKCjWJCkrJb9"
   },
   "source": [
    "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "UvcgbHf1rIXZ"
   },
   "outputs": [],
   "source": [
    "workflow.add_edge(\"action\", \"agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EiWDwBQtrw7Z"
   },
   "source": [
    "Let's look at the final visualization.\n",
    "\n",
    "![image](https://i.imgur.com/NWO7usO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYqDpErlsCsu"
   },
   "source": [
    "All that's left to do now is to compile our workflow - and we're off!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "zt9-KS8DpzNx"
   },
   "outputs": [],
   "source": [
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhNWIwBL1W4Q"
   },
   "source": [
    "#### ‚ùì Question #2:\n",
    "\n",
    "Is there any specific limit to how many times we can cycle?\n",
    "\n",
    "If not, how could we impose a limit to the number of cycles?\n",
    "\n",
    "---\n",
    "***ANSWER:***\n",
    "\n",
    "There is a parameter called \"recursion_limit\", which can be passed to the graph when invoking it. Otherwise it can also be managed manually using our state variable and a conditional edge that would check how many times recursion occurred and limit it.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSCds6zTL5VJ"
   },
   "source": [
    "#### Helper Function to print messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xRPF0X5iL8Bh"
   },
   "outputs": [],
   "source": [
    "def print_messages(messages):\n",
    "  next_is_tool = False\n",
    "  initial_query = True\n",
    "  for message in messages[\"messages\"]:\n",
    "    if \"function_call\" in message.additional_kwargs:\n",
    "      print()\n",
    "      print(f'Tool Call - Name: {message.additional_kwargs[\"function_call\"][\"name\"]} + Query: {message.additional_kwargs[\"function_call\"][\"arguments\"]}')\n",
    "      next_is_tool = True\n",
    "      continue\n",
    "    if next_is_tool:\n",
    "      print(f\"Tool Response: {message.content}\")\n",
    "      next_is_tool = False\n",
    "      continue\n",
    "    if initial_query:\n",
    "      print(f\"Initial Query: {message.content}\")\n",
    "      print()\n",
    "      initial_query = False\n",
    "      continue\n",
    "    print()\n",
    "    print(f\"Agent Response: {message.content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEYcTShCsPaa"
   },
   "source": [
    "## Using Our Graph\n",
    "\n",
    "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
    "\n",
    "Let's try out a few examples to see how it fairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qn4n37PQRPII",
    "outputId": "90f7d3dc-0fe2-4d1f-8221-9b3bcffc982e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Query: What is RAG in the context of Large Language Models? When did it break onto the scene?\n",
      "\n",
      "\n",
      "Agent Response: RAG, or Retrieval-Augmented Generation, is a technique used in the context of large language models (LLMs) to improve their performance by combining retrieval-based methods with generative models. The core idea behind RAG is to enhance the generative capabilities of language models by incorporating relevant information retrieved from a large corpus of documents or a knowledge base.\n",
      "\n",
      "### Key Components of RAG:\n",
      "1. **Retriever**: This component is responsible for fetching relevant documents or passages from a large corpus based on the input query.\n",
      "2. **Generator**: This component generates the final response by conditioning on both the input query and the retrieved documents.\n",
      "\n",
      "### How RAG Works:\n",
      "1. **Query Input**: The user provides a query or prompt.\n",
      "2. **Document Retrieval**: The retriever searches a large corpus to find documents or passages that are relevant to the query.\n",
      "3. **Response Generation**: The generator uses the retrieved documents along with the original query to generate a coherent and contextually accurate response.\n",
      "\n",
      "### Advantages of RAG:\n",
      "- **Improved Accuracy**: By leveraging external knowledge, RAG can provide more accurate and contextually relevant responses.\n",
      "- **Scalability**: It can handle a vast amount of information by retrieving only the most relevant pieces, making it scalable for large datasets.\n",
      "- **Flexibility**: It can be adapted to various domains by changing the underlying corpus used for retrieval.\n",
      "\n",
      "### When Did RAG Break Onto the Scene?\n",
      "RAG was introduced by Facebook AI Research (FAIR) in a paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\" which was published in 2020. The paper demonstrated that RAG could significantly improve the performance of language models on various knowledge-intensive tasks, such as open-domain question answering and fact-checking.\n",
      "\n",
      "Would you like more detailed information or specific aspects of RAG?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\" : [HumanMessage(content=\"What is RAG in the context of Large Language Models? When did it break onto the scene?\")]}\n",
    "\n",
    "messages = app.invoke(inputs)\n",
    "\n",
    "print_messages(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBHnUtLSscRr"
   },
   "source": [
    "Let's look at what happened:\n",
    "\n",
    "1. Our state object was populated with our request\n",
    "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
    "3. The conditional edge received the state object, found the \"function_call\" `additional_kwarg`, and sent the state object to the action node\n",
    "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
    "5. The agent node added a response to the state object and passed it along the conditional edge\n",
    "6. The conditional edge received the state object, could not find the \"function_call\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
    "\n",
    "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afv2BuEsV5JG",
    "outputId": "6ae3afec-7da7-4c5d-a3ca-2ed75735c029",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Query: What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? \n",
      "Once you have that information, can you look up the bio of the first author on the QLoRA paper?\n",
      "\n",
      "\n",
      "Tool Call - Name: arxiv + Query: {\"query\":\"QLoRA\"}\n",
      "Tool Response: Published: 2023-05-23\n",
      "Title: QLoRA: Efficient Finetuning of Quantized LLMs\n",
      "Authors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\n",
      "Summary: We present QLoRA, an efficient finetuning approach that reduces memory usage\n",
      "enough to finetune a 65B parameter model on a single 48GB GPU while preserving\n",
      "full 16-bit finetuning task performance. QLoRA backpropagates gradients through\n",
      "a frozen, 4-bit quantized pretrained language model into Low Rank\n",
      "Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\n",
      "previous openly released models on the Vicuna benchmark, reaching 99.3% of the\n",
      "performance level of ChatGPT while only requiring 24 hours of finetuning on a\n",
      "single GPU. QLoRA introduces a number of innovations to save memory without\n",
      "sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\n",
      "information theoretically optimal for normally distributed weights (b) double\n",
      "quantization to reduce the average memory footprint by quantizing the\n",
      "quantization constants, and (c) paged optimziers to manage memory spikes. We\n",
      "use QLoRA to finetune more than 1,000 models, providing a detailed analysis of\n",
      "instruction following and chatbot performance across 8 instruction datasets,\n",
      "multiple model types (LLaMA, T5), and model scales that would be infeasible to\n",
      "run with regular finetuning (e.g. 33B and 65B parameter models). Our results\n",
      "show that QLoRA finetuning on a small high-quality dataset leads to\n",
      "state-of-the-art results, even when using smaller models than the previous\n",
      "SoTA. We provide a detailed analysis of chatbot performance based on both human\n",
      "and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\n",
      "alternative to human evaluation. Furthermore, we find that current chatbot\n",
      "benchmarks are not trustworthy to accurately evaluate the performance levels of\n",
      "chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\n",
      "ChatGPT. We release all of our models and code, including CUDA kernels for\n",
      "4-bit training.\n",
      "\n",
      "Published: 2024-05-27\n",
      "Title: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\n",
      "Authors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\n",
      "Summary: The LoRA-finetuning quantization of LLMs has been extensively studied to\n",
      "obtain accurate yet compact LLMs for deployment on resource-constrained\n",
      "hardware. However, existing methods cause the quantized LLM to severely degrade\n",
      "and even fail to benefit from the finetuning of LoRA. This paper proposes a\n",
      "novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\n",
      "through information retention. The proposed IR-QLoRA mainly relies on two\n",
      "technologies derived from the perspective of unified information: (1)\n",
      "statistics-based Information Calibration Quantization allows the quantized\n",
      "parameters of LLM to retain original information accurately; (2)\n",
      "finetuning-based Information Elastic Connection makes LoRA utilizes elastic\n",
      "representation transformation with diverse information. Comprehensive\n",
      "experiments show that IR-QLoRA can significantly improve accuracy across LLaMA\n",
      "and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\n",
      "improvement on MMLU compared with the state-of-the-art methods. The significant\n",
      "performance gain requires only a tiny 0.31% additional time consumption,\n",
      "revealing the satisfactory efficiency of our IR-QLoRA. We highlight that\n",
      "IR-QLoRA enjoys excellent versatility, compatible with various frameworks\n",
      "(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\n",
      "The code is available at https://github.com/htqin/ir-qlora.\n",
      "\n",
      "Published: 2024-06-12\n",
      "Title: Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods\n",
      "Authors: Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk\n",
      "Summary: There are various methods for adapting LLMs to different domains. The most\n",
      "common methods are prompting, finetuning, and RAG. In this w\n",
      "\n",
      "Tool Call - Name: duckduckgo_search + Query: {\"query\":\"Tim Dettmers biography\"}\n",
      "Tool Response: Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Speaker Biography. Tim Dettmers' research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio. Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ...\n",
      "\n",
      "Agent Response: ### What is QLoRA in Machine Learning?\n",
      "\n",
      "QLoRA (Quantized Low Rank Adapters) is an efficient finetuning approach designed to reduce memory usage while preserving the performance of large language models (LLMs). The key innovation of QLoRA is its ability to backpropagate gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). This allows for the finetuning of large models, such as a 65 billion parameter model, on a single 48GB GPU without sacrificing performance.\n",
      "\n",
      "Key features of QLoRA include:\n",
      "- **4-bit NormalFloat (NF4)**: A new data type optimized for normally distributed weights.\n",
      "- **Double Quantization**: Reduces memory footprint by quantizing the quantization constants.\n",
      "- **Paged Optimizers**: Manages memory spikes during training.\n",
      "\n",
      "The approach has been shown to outperform previous models on benchmarks like Vicuna, achieving 99.3% of the performance level of ChatGPT with only 24 hours of finetuning on a single GPU.\n",
      "\n",
      "### Technical Papers on QLoRA\n",
      "\n",
      "1. **Title**: QLoRA: Efficient Finetuning of Quantized LLMs\n",
      "   - **Authors**: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\n",
      "   - **Published**: 2023-05-23\n",
      "   - **Summary**: This paper introduces QLoRA and its innovations, providing a detailed analysis of its performance across various models and datasets. The authors also discuss the limitations of current chatbot benchmarks and propose improvements.\n",
      "\n",
      "2. **Title**: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\n",
      "   - **Authors**: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\n",
      "   - **Published**: 2024-05-27\n",
      "   - **Summary**: This paper proposes IR-QLoRA, an extension of QLoRA that focuses on information retention to improve the accuracy of quantized LLMs. The authors present comprehensive experiments showing significant performance gains.\n",
      "\n",
      "3. **Title**: Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods\n",
      "   - **Authors**: Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk\n",
      "   - **Published**: 2024-06-12\n",
      "   - **Summary**: This paper explores the application of QLoRA in adapting LLMs to different domains, focusing on fact memorization and style imitation.\n",
      "\n",
      "### Biography of Tim Dettmers\n",
      "\n",
      "Tim Dettmers is a researcher whose work focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. His research involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cost-effective deep learning.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\" : [HumanMessage(content=\"\"\"What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? \n",
    "Once you have that information, can you look up the bio of the first author on the QLoRA paper?\"\"\")]}\n",
    "\n",
    "messages = app.invoke(inputs)\n",
    "\n",
    "print_messages(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXzDlZVz1Hnf"
   },
   "source": [
    "####üèóÔ∏è Activity #2:\n",
    "\n",
    "Please write out the steps the agent took to arrive at the correct answer.\n",
    "\n",
    "---\n",
    "**ANSWER:**\n",
    "\n",
    "Once the agent received the query, it decided that it needed to search for technical papers in ArXiV. It made a tool call for ArXiV with the query \"QLoRA\" and retrieved 3 papers. \n",
    "\n",
    "Then, it decided that it needed to search for the biographies of the authors, so it again made a tool call, this time in DuckDuckGo, with the query \"Tim Dettmers biography\". \n",
    "\n",
    "With these two results, it was satisfied and finally composed its final answer, which includes an explanation of what is QLoRA, listed its most important paper, and gave a summary of the biography of the first author of the paper.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQmrzYfrm1Dr"
   },
   "source": [
    "# ü§ù Breakout Room #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7c8-Uyarh1v"
   },
   "source": [
    "## Part 1: LangSmith Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pV3XeFOT1Sar"
   },
   "source": [
    "### Pre-processing for LangSmith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wruQCuzewUuO"
   },
   "source": [
    "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "oeXdQgbxwhTv"
   },
   "outputs": [],
   "source": [
    "def convert_inputs(input_object):\n",
    "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
    "\n",
    "def parse_output(input_state):\n",
    "  return input_state[\"messages\"][-1].content\n",
    "\n",
    "agent_chain = convert_inputs | app | parse_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "orYxBZXSxJjZ",
    "outputId": "d2b3be9d-1dbb-4109-83e4-673c9b73483e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"RAG stands for Retrieval-Augmented Generation. It is a technique used in natural language processing (NLP) and machine learning to improve the performance of language models by combining retrieval-based methods with generative models. Here's a brief overview of how it works:\\n\\n1. **Retrieval**: In the first step, the system retrieves relevant documents or pieces of information from a large corpus based on the input query. This is typically done using a retrieval model, such as BM25 or a dense retrieval model like DPR (Dense Passage Retrieval).\\n\\n2. **Augmentation**: The retrieved documents are then used to augment the input query. This can involve concatenating the retrieved information with the original query or using it to provide additional context.\\n\\n3. **Generation**: Finally, a generative model, such as a transformer-based language model (e.g., GPT-3, BERT), is used to generate a response based on the augmented input. The generative model can produce more accurate and contextually relevant responses by leveraging the additional information provided by the retrieval step.\\n\\nRAG models are particularly useful in scenarios where the information needed to answer a query is not contained within the model's parameters but can be found in external documents. This approach allows the model to access a broader knowledge base and generate more informed and accurate responses.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.invoke({\"question\" : \"What is RAG?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9UkCIqkpyZu"
   },
   "source": [
    "### Task 1: Creating An Evaluation Dataset\n",
    "\n",
    "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
    "\n",
    "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
    "\n",
    "```python\n",
    "questions = [\n",
    "    \"What optimizer is used in QLoRA?\",\n",
    "    \"What data type was created in the QLoRA paper?\",\n",
    "    \"What is a Retrieval Augmented Generation system?\",\n",
    "    \"Who authored the QLoRA paper?\",\n",
    "    \"What is the most popular deep learning framework?\",\n",
    "    \"What significant improvements does the LoRA system make?\"\n",
    "]\n",
    "\n",
    "answers = [\n",
    "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
    "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
    "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
    "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
    "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
    "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfMXF2KAsQxs"
   },
   "source": [
    "####üèóÔ∏è Activity #3:\n",
    "\n",
    "Please create a dataset in the above format with at least 5 questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "CbagRuJop83E"
   },
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What optimizer is used in QLoRA?\",\n",
    "    \"What data type was created in the QLoRA paper?\",\n",
    "    \"What is a Retrieval Augmented Generation system?\",\n",
    "    \"Who authored the QLoRA paper?\",\n",
    "    \"What is the most popular deep learning framework?\",\n",
    "    \"What significant improvements does the LoRA system make?\"\n",
    "]\n",
    "\n",
    "answers = [\n",
    "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
    "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
    "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
    "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
    "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
    "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7QVFuAmsh7L"
   },
   "source": [
    "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "RLfrZrgSsn85"
   },
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
    ")\n",
    "\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\" : q} for q in questions],\n",
    "    outputs=answers,\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciV73F9Q04w0"
   },
   "source": [
    "#### ‚ùì Question #3:\n",
    "\n",
    "How are the correct answers associated with the questions?\n",
    "\n",
    "> NOTE: Feel free to indicate if this is problematic or not\n",
    "\n",
    "---\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "With the dataset above, it looks like it's just the order of the elements, that is, by index. For example, question[0] is related to answer[0]. For some simple evaluation system, the ordering should be enough, but for more complex evaluation strategies it would be good to have a more robust way of defining questions and their answers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lRTXUrTtP9Y"
   },
   "source": [
    "### Task 2: Adding Evaluators\n",
    "\n",
    "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
    "\n",
    "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "QrAUXMFftlAY"
   },
   "outputs": [],
   "source": [
    "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
    "\n",
    "@run_evaluator\n",
    "def must_mention(run, example) -> EvaluationResult:\n",
    "    prediction = run.outputs.get(\"output\") or \"\"\n",
    "    required = example.outputs.get(\"must_mention\") or []\n",
    "    score = all(phrase in prediction for phrase in required)\n",
    "    return EvaluationResult(key=\"must_mention\", score=score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNtHORUh0jZY"
   },
   "source": [
    "#### ‚ùì Question #4:\n",
    "\n",
    "What are some ways you could improve this metric as-is?\n",
    "\n",
    "> NOTE: Alternatively you can suggest where gaps exist in this method.\n",
    "\n",
    "This is a very naive implementation of the check whether an answer contains a required term. This would fail in case of different cases (e.g. Tensorflow instead of TensorFlow) or simple spelling mistakes. A simple improvement would be to convert everything to lower case before comparing; another improvement could be to generate alternate spellings of the required answer and consider them, too. One possible way could be to do a comparison by embeddings, although I'm not sure if this would then be too relaxed with the must-mention requirement. Finally, an LLM-based evaluator that judges whether the must-mention terms are in the answer would potentially be even more able to catch right answers that don't exactly match the term.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZ4DVSXl0BX5"
   },
   "source": [
    "Now that we have created our custom evaluator - let's initialize our `RunEvalConfig` with it, and a few others:\n",
    "\n",
    "- `\"criteria\"` includes the default criteria which, in this case, means \"helpfulness\"\n",
    "- `\"cot_qa\"` includes a criteria that bases whether or not the answer is correct by utilizing a Chain of Thought prompt and the provided context to determine if the response is correct or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "sL4-XcjytWsu"
   },
   "outputs": [],
   "source": [
    "from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "\n",
    "eval_config = RunEvalConfig(\n",
    "    custom_evaluators=[must_mention],\n",
    "    evaluators=[\n",
    "        \"criteria\",\n",
    "        \"cot_qa\",\n",
    "    ],\n",
    "    eval_llm=ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1RJr349zhv7"
   },
   "source": [
    "Task 3: Evaluating\n",
    "\n",
    "All that is left to do is evaluate our agent's response!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p5TeCUUkuGld",
    "outputId": "157f1019-4ee4-4994-fe80-7afc2e5fdfb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'RAG Pipeline - Evaluation - 06dfe28f' at:\n",
      "https://smith.langchain.com/o/fcaff44a-5186-52ee-9403-69aa238eb874/datasets/d3059a95-6517-4632-a606-fbcd40c4334b/compare?selectedSessions=a67d15fa-9b29-4105-a3ec-f7949d4ed533\n",
      "\n",
      "View all tests for Dataset Retrieval Augmented Generation - Evaluation Dataset - 0a4c0125 at:\n",
      "https://smith.langchain.com/o/fcaff44a-5186-52ee-9403-69aa238eb874/datasets/d3059a95-6517-4632-a606-fbcd40c4334b\n",
      "[------------------------------------------------->] 6/6"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'project_name': 'RAG Pipeline - Evaluation - 06dfe28f',\n",
       " 'results': {'298953f1-aae1-4d9a-a0ce-9e86db2d69b2': {'input': {'question': 'What optimizer is used in QLoRA?'},\n",
       "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='To determine if the submission meets the criteria of helpfulness, I will evaluate it based on the following aspects:\\n\\n1. **Helpfulness**: Does the submission provide a clear and accurate answer to the question asked?\\n2. **Insightfulness**: Does the submission offer additional useful information or context that enhances understanding?\\n3. **Appropriateness**: Is the submission relevant and directly addresses the question without unnecessary information?\\n\\n### Step-by-Step Evaluation:\\n\\n1. **Helpfulness**:\\n   - The question asks specifically about the optimizer used in QLoRA.\\n   - The submission mentions \"paged optimizers\" as a key innovation in QLoRA for managing memory spikes.\\n   - The term \"paged optimizers\" is not a standard term widely recognized in the context of optimizers for machine learning models. It seems to refer more to a memory management technique rather than a specific optimizer algorithm like Adam, SGD, etc.\\n   - The submission does not clearly state a specific optimizer algorithm used in QLoRA, which is what the question is likely seeking.\\n\\n2. **Insightfulness**:\\n   - The submission provides some context about memory management during fine-tuning, which is somewhat insightful.\\n   - However, it does not offer additional useful information about the specific optimizer algorithm used, which is the core of the question.\\n\\n3. **Appropriateness**:\\n   - The submission is relevant to the topic of QLoRA and discusses aspects of optimization.\\n   - However, it does not directly answer the specific question about the optimizer used, making it less appropriate for the given task.\\n\\n### Conclusion:\\nThe submission does not clearly and directly answer the specific question about the optimizer used in QLoRA. It provides some related information but fails to address the core of the question accurately.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('298dc5db-531c-48d1-a4e8-a59813e5837d'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
       "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment='EXPLANATION:\\n1. The question asks specifically about the optimizer used in QLoRA.\\n2. The context provided includes the terms \\'paged\\' and \\'optimizer\\'.\\n3. The student\\'s answer explains that QLoRA uses \"paged optimizers\" to manage memory spikes and optimize memory usage and performance during fine-tuning.\\n4. The student\\'s answer aligns with the context provided, which mentions \\'paged\\' and \\'optimizer\\'.\\n5. The student\\'s answer does not contain any conflicting statements and accurately describes the use of paged optimizers in QLoRA.\\n\\nGRADE: CORRECT', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('b9491f87-dce6-4df6-80c6-5ca3cedfdcc9'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
       "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('b0125049-2e60-4b9b-9023-cfdad4e84a79'), target_run_id=None)],\n",
       "   'execution_time': 6.617997,\n",
       "   'run_id': '407333f4-dc22-447c-bbbf-b3be79479e7a',\n",
       "   'output': 'QLoRA (Quantized Low-Rank Adaptation) uses a combination of techniques to optimize memory usage and performance during the fine-tuning of large language models. One of the key innovations mentioned in the QLoRA paper is the use of \"paged optimizers\" to manage memory spikes. This approach helps in efficiently handling the memory requirements during the fine-tuning process, especially when dealing with large models. \\n\\nIn summary, QLoRA employs paged optimizers as part of its strategy to optimize memory usage and maintain performance during the fine-tuning of quantized large language models.',\n",
       "   'reference': {'must_mention': ['paged', 'optimizer']}},\n",
       "  '5e5d34d5-1929-4ec1-8c73-8e4003c090ee': {'input': {'question': 'What data type was created in the QLoRA paper?'},\n",
       "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment=\"To determine if the submission meets the criteria of helpfulness, insightfulness, and appropriateness, let's evaluate each aspect step by step:\\n\\n1. **Helpfulness**:\\n   - The submission directly answers the question posed in the input by identifying the data type created in the QLoRA paper.\\n   - It provides additional context about the data type, explaining its purpose and benefits, which adds value to the response.\\n\\n2. **Insightfulness**:\\n   - The submission goes beyond merely naming the data type by explaining why it was created and how it is beneficial. This shows a deeper understanding of the topic and provides the reader with more comprehensive information.\\n\\n3. **Appropriateness**:\\n   - The submission is relevant to the question asked and stays on topic.\\n   - The language used is clear and professional, making it suitable for the context.\\n\\nBased on this evaluation, the submission meets all the criteria of being helpful, insightful, and appropriate.\\n\\nY\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a9edc028-b328-4d80-bbeb-aac2de2c180c'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
       "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment='EXPLANATION:\\n1. The question asks for the data type created in the QLoRA paper.\\n2. The context provided lists two data types: \\'NF4\\' and \\'NormalFloat\\'.\\n3. The student\\'s answer states that the data type introduced is \"4-bit NormalFloat (NF4)\".\\n4. The student\\'s answer also provides additional information about the purpose and benefits of this data type, but this extra information does not conflict with the context.\\n5. The key part of the student\\'s answer, \"4-bit NormalFloat (NF4)\", matches the context provided (\\'NF4\\' and \\'NormalFloat\\').\\n\\nSince the student\\'s answer correctly identifies the data type as \"4-bit NormalFloat (NF4)\" and does not contain any conflicting information, the answer is factually accurate.\\n\\nGRADE: CORRECT', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('163edcaa-5e7c-41fe-be4d-6e49ba94bd0e'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
       "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('c140c703-f24b-45c2-ba60-76dfd99ad134'), target_run_id=None)],\n",
       "   'execution_time': 9.010814,\n",
       "   'run_id': '0ba4f1ac-c69c-4261-9411-9217bcf7dcc9',\n",
       "   'output': 'In the QLoRA paper, the authors introduced a new data type called **4-bit NormalFloat (NF4)**. This data type is designed to be information-theoretically optimal for normally distributed weights, which helps in reducing memory usage while preserving performance during the finetuning of large language models.',\n",
       "   'reference': {'must_mention': ['NF4', 'NormalFloat']}},\n",
       "  '9303f47b-8fb1-40b7-a4a3-befe0c9b4689': {'input': {'question': 'What is a Retrieval Augmented Generation system?'},\n",
       "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment=\"To determine if the submission meets the criteria of helpfulness, insightfulness, and appropriateness, let's break down each aspect:\\n\\n1. **Helpfulness**:\\n   - The submission provides a clear and detailed explanation of what a Retrieval-Augmented Generation (RAG) system is.\\n   - It breaks down the components of a RAG system (Retrieval Component, Generation Component, and Integration) and explains how they work together.\\n   - It lists the benefits of RAG systems, which helps the reader understand why such systems are useful.\\n   - It provides examples of applications, which helps contextualize the information and shows practical uses of RAG systems.\\n\\n2. **Insightfulness**:\\n   - The submission goes beyond a basic definition by explaining the mechanisms and benefits of RAG systems.\\n   - It mentions specific techniques used in the retrieval component (e.g., TF-IDF, BM25, neural retrieval models), which adds depth to the explanation.\\n   - It discusses how the integration of retrieved documents can enhance the generation process, providing a nuanced understanding of the system's functionality.\\n\\n3. **Appropriateness**:\\n   - The language and terminology used are appropriate for the topic and likely audience, assuming the reader has some familiarity with AI and machine learning concepts.\\n   - The structure of the submission is logical and easy to follow, with clear headings and bullet points that enhance readability.\\n   - The content is relevant to the input query, providing a comprehensive answer to what a RAG system is.\\n\\nBased on this analysis, the submission meets all the criteria of being helpful, insightful, and appropriate.\\n\\nY\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('cc6f6c95-359d-4582-bac5-78e85f6691af'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
       "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"EXPLANATION:\\n1. **Identify the key components of a Retrieval-Augmented Generation (RAG) system**:\\n   - The system combines retrieval-based and generation-based approaches.\\n   - It has a retrieval component that retrieves relevant documents.\\n   - It has a generation component that generates text based on the retrieved documents and input query.\\n   - The integration of these components enhances the quality and relevance of the generated text.\\n\\n2. **Check the student's answer for these components**:\\n   - The student mentions that a RAG system combines retrieval-based and generation-based approaches.\\n   - The retrieval component retrieves relevant documents using techniques like TF-IDF, BM25, or neural retrieval models.\\n   - The generation component generates text using sequence-to-sequence models like Transformer-based models (e.g., GPT-3, BERT).\\n   - The integration of retrieved documents with the input query to produce more accurate and informative responses is described.\\n\\n3. **Evaluate the additional information provided by the student**:\\n   - The student elaborates on the benefits of RAG systems, such as improved relevance, knowledge integration, and contextual understanding.\\n   - The student also provides examples of applications, including question answering, customer support, and content creation.\\n\\n4. **Determine if the student's answer contains any conflicting statements**:\\n   - The student's answer does not contain any conflicting statements with the context provided.\\n\\n5. **Conclusion**:\\n   - The student's answer accurately describes a Retrieval-Augmented Generation system, including its components, benefits, and applications.\\n   - The additional information provided by the student is relevant and does not conflict with the context.\\n\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('3500e62d-5624-41e9-a8e7-cd2be5990179'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
       "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('bf94ae00-4108-415a-8b5e-a94a57d08599'), target_run_id=None)],\n",
       "   'execution_time': 10.208942,\n",
       "   'run_id': '15a6d087-e9c4-4d04-b881-e136c0c8e165',\n",
       "   'output': 'A Retrieval-Augmented Generation (RAG) system is a type of artificial intelligence model that combines retrieval-based and generation-based approaches to improve the quality and relevance of generated text. Here‚Äôs a breakdown of the components and how they work together:\\n\\n1. **Retrieval Component**:\\n   - This part of the system retrieves relevant documents or pieces of information from a large corpus or database based on the input query.\\n   - It uses techniques from information retrieval, such as TF-IDF, BM25, or more advanced neural retrieval models, to find the most relevant documents.\\n\\n2. **Generation Component**:\\n   - This part of the system generates text based on the retrieved documents and the input query.\\n   - It typically uses a sequence-to-sequence model, such as a Transformer-based model (e.g., GPT-3, BERT), to generate coherent and contextually appropriate responses.\\n\\n3. **Integration**:\\n   - The retrieved documents provide additional context and information that the generation model can use to produce more accurate and informative responses.\\n   - The system can be designed to either concatenate the retrieved documents with the input query before feeding them into the generation model or to use a more sophisticated mechanism to integrate the retrieved information.\\n\\n### Benefits of RAG Systems:\\n- **Improved Relevance**: By leveraging external documents, the system can generate responses that are more relevant and factually accurate.\\n- **Knowledge Integration**: It can incorporate up-to-date information from a large corpus, making it more knowledgeable about recent events or specific domains.\\n- **Contextual Understanding**: The retrieval component helps the generation model understand the context better, leading to more coherent and contextually appropriate responses.\\n\\n### Applications:\\n- **Question Answering**: Providing detailed and accurate answers to user queries by retrieving relevant documents and generating responses based on them.\\n- **Customer Support**: Assisting in customer service by retrieving relevant information from a knowledge base and generating helpful responses.\\n- **Content Creation**: Aiding in the creation of articles, reports, or other content by retrieving relevant information and generating text based on it.\\n\\nOverall, RAG systems represent a powerful approach to combining the strengths of retrieval-based and generation-based models to produce high-quality, informative, and contextually appropriate text.',\n",
       "   'reference': {'must_mention': ['ground', 'context']}},\n",
       "  '6f0d7fdd-0cc0-4ce3-9d51-72f8b78c8b6f': {'input': {'question': 'Who authored the QLoRA paper?'},\n",
       "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='To determine if the submission meets the criteria of helpfulness, I will evaluate it based on the following aspects:\\n\\n1. **Helpfulness**: Does the submission provide the information requested in the input?\\n   - The input asks for the authors of the QLoRA paper.\\n   - The submission lists the authors of the QLoRA paper.\\n   - The submission also provides the title of the paper and a link to access it, which adds value.\\n\\n2. **Insightfulness**: Does the submission offer any additional useful information beyond the basic requirement?\\n   - The submission includes the full title of the paper, which can help in identifying the correct document.\\n   - The submission provides a link to the paper, which is useful for further reading and verification.\\n\\n3. **Appropriateness**: Is the submission relevant and suitable for the input query?\\n   - The submission directly addresses the input query by listing the authors.\\n   - The additional information (title and link) is relevant and enhances the response.\\n\\nBased on the above evaluation, the submission is helpful, insightful, and appropriate. Therefore, it meets all the criteria.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('c967166b-3c66-4106-bbd5-892e8871af1e'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
       "    EvaluationResult(key='COT Contextual Accuracy', score=0, value='INCORRECT', comment='EXPLANATION:\\n1. The question asks for the author(s) of the QLoRA paper.\\n2. The context provided lists \"Tim\" and \"Dettmers\" as the authors.\\n3. The student\\'s answer lists a different set of authors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, and Michele Magno.\\n4. There is a discrepancy between the context and the student\\'s answer. The context does not mention any of the names listed by the student.\\n5. Therefore, the student\\'s answer does not match the information provided in the context.\\n\\nGRADE: INCORRECT', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('e9db8262-1dce-4d6b-8c87-f5df75dfe53d'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
       "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('1411da13-6203-4d64-8ab7-854fac0ae24b'), target_run_id=None)],\n",
       "   'execution_time': 6.216411,\n",
       "   'run_id': '4c2d22c7-8b6a-4edd-be16-5faf9ed2220a',\n",
       "   'output': 'The QLoRA paper titled \"Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\" was authored by:\\n\\n- Haotong Qin\\n- Xudong Ma\\n- Xingyu Zheng\\n- Xiaoyang Li\\n- Yang Zhang\\n- Shouda Liu\\n- Jie Luo\\n- Xianglong Liu\\n- Michele Magno\\n\\nYou can find more details and access the paper [here](https://github.com/htqin/ir-qlora).',\n",
       "   'reference': {'must_mention': ['Tim', 'Dettmers']}},\n",
       "  'c869da7e-012c-4f73-a180-0771ee40482f': {'input': {'question': 'What is the most popular deep learning framework?'},\n",
       "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment=\"To determine if the submission meets the criteria of helpfulness, I will evaluate it based on the following aspects:\\n\\n1. **Helpfulness**: Does the submission provide useful information that addresses the input question?\\n2. **Insightfulness**: Does the submission offer a deeper understanding or additional context that enhances the reader's knowledge?\\n3. **Appropriateness**: Is the submission relevant and suitable for the input question?\\n\\n### Step-by-Step Evaluation:\\n\\n1. **Helpfulness**:\\n   - The input question asks for the most popular deep learning framework.\\n   - The submission lists three frameworks: TensorFlow, PyTorch, and Keras.\\n   - It provides a brief description of each framework, including their developers, key features, and reasons for their popularity.\\n   - This information is useful for someone looking to understand which deep learning frameworks are popular and why.\\n\\n2. **Insightfulness**:\\n   - The submission offers additional context by mentioning the developers of each framework (Google, Facebook's AI Research lab, and Fran√ßois Chollet).\\n   - It highlights specific strengths of each framework, such as TensorFlow's numerical computation capabilities, PyTorch's ease of use and debugging tools, and Keras's simplicity.\\n   - This added context helps the reader understand not just which frameworks are popular, but also why they are preferred in different scenarios.\\n\\n3. **Appropriateness**:\\n   - The submission is directly relevant to the input question.\\n   - It stays on topic and provides a clear, concise answer without deviating into unrelated information.\\n\\nBased on the above evaluation, the submission meets all the criteria of being helpful, insightful, and appropriate.\\n\\nY\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('26216ebf-637f-41f3-8451-fe920b278a25'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
       "    EvaluationResult(key='COT Contextual Accuracy', score=0, value='INCORRECT', comment='EXPLANATION:\\n1. The question asks for the \"most popular deep learning framework.\"\\n2. The context provided lists two frameworks: \\'PyTorch\\' and \\'TensorFlow.\\'\\n3. The student\\'s answer lists three frameworks: TensorFlow, PyTorch, and Keras.\\n4. The student\\'s answer provides detailed descriptions of each framework but does not directly answer the question of which single framework is the most popular.\\n5. The student\\'s answer does not explicitly state which one of the frameworks is the most popular, which is the core requirement of the question.\\n6. The additional information about Keras is not relevant to the context provided and does not help in determining the most popular framework.\\n\\nGRADE: INCORRECT', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('4af37586-b6f6-44d2-ad89-172ae6ed7454'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
       "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('04c9470f-d9de-4e32-b59e-5af75102b2c2'), target_run_id=None)],\n",
       "   'execution_time': 5.9589,\n",
       "   'run_id': 'c86090a2-0ca6-439e-8de9-cfbbd1c339d0',\n",
       "   'output': \"As of 2023, the most popular deep learning frameworks are:\\n\\n1. **TensorFlow**: Developed by Google, TensorFlow is one of the most widely used open-source libraries for numerical computation and deep learning. It is heavily utilized by major corporations and has a large, active community.\\n\\n2. **PyTorch**: Developed by Facebook's AI Research lab, PyTorch is the preferred framework for research and development. It is known for its ease of use, GPU capabilities, scalability, and excellent debugging tools.\\n\\n3. **Keras**: Initially developed by Fran√ßois Chollet, Keras is a high-level neural network API written in Python. It runs on top of TensorFlow, Theano, and CNTK, and is known for its simplicity and ease of use.\\n\\nThese frameworks are popular due to their robust features, active community support, and widespread adoption in both industry and academia.\",\n",
       "   'reference': {'must_mention': ['PyTorch', 'TensorFlow']}},\n",
       "  '9a1d3d40-783f-42b5-b923-22204253fcc9': {'input': {'question': 'What significant improvements does the LoRA system make?'},\n",
       "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment=\"To determine if the submission meets the criteria of being helpful, insightful, and appropriate, let's break down each aspect:\\n\\n1. **Helpfulness**:\\n   - The submission provides a clear and detailed explanation of the significant improvements made by the LoRA system.\\n   - It lists specific benefits such as parameter efficiency, memory efficiency, faster training, better generalization, scalability, modularity, and cost-effectiveness.\\n   - Each point is explained in a way that is easy to understand, making the information accessible to readers who may not be experts in the field.\\n\\n2. **Insightfulness**:\\n   - The submission goes beyond just listing improvements; it provides context and reasoning for why these improvements are significant.\\n   - It explains how LoRA achieves these benefits (e.g., by introducing low-rank matrices) and why they are important in the context of fine-tuning large pre-trained models.\\n   - The insights into how LoRA can help avoid overfitting and improve generalization are particularly valuable.\\n\\n3. **Appropriateness**:\\n   - The submission stays on topic and directly addresses the input question about the significant improvements made by the LoRA system.\\n   - The language and tone are appropriate for a technical explanation, making it suitable for an audience interested in machine learning.\\n   - There are no irrelevant details or digressions, ensuring that the response is focused and concise.\\n\\nBased on this analysis, the submission meets all the criteria of being helpful, insightful, and appropriate.\\n\\nY\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('0fb17017-c2a2-4b76-88e9-f13799540a32'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
       "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment='EXPLANATION:\\n1. The question asks for significant improvements made by the LoRA system.\\n2. The context provided is \"reduce\" and \"parameters,\" which suggests that the improvements should be related to reducing parameters.\\n3. The student\\'s answer lists several improvements, including:\\n   - Parameter Efficiency: Reduces the number of trainable parameters.\\n   - Memory Efficiency: Reduces memory footprint.\\n   - Faster Training: Fewer parameters to update.\\n   - Better Generalization: Avoids overfitting.\\n   - Scalability: Feasible to fine-tune very large models.\\n   - Modularity: Low-rank matrices as modular components.\\n   - Cost-Effectiveness: Reduces computational and memory requirements.\\n\\n4. The first point, \"Parameter Efficiency,\" directly addresses the context of reducing parameters.\\n5. The other points, while not directly mentioned in the context, do not conflict with the context and provide additional relevant information about the benefits of LoRA.\\n6. The student\\'s answer does not contain any conflicting statements and aligns well with the context provided.\\n\\nGRADE: CORRECT', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('999ed15e-b509-4e76-a1f5-0bb958d5c355'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
       "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('3bfcbe93-61e5-4fbd-b107-88f5f21120b3'), target_run_id=None)],\n",
       "   'execution_time': 6.29854,\n",
       "   'run_id': '3816ec2f-ac5b-425c-b398-0da6965e3b2e',\n",
       "   'output': 'LoRA, or Low-Rank Adaptation, is a technique used in the field of machine learning, particularly in the context of fine-tuning large pre-trained models. Here are some significant improvements that the LoRA system makes:\\n\\n1. **Parameter Efficiency**: LoRA significantly reduces the number of trainable parameters required for fine-tuning. Instead of updating all the parameters of a large model, LoRA introduces low-rank matrices that adapt the pre-trained weights, making the process more efficient.\\n\\n2. **Memory Efficiency**: By reducing the number of parameters that need to be updated, LoRA also reduces the memory footprint during training. This is particularly beneficial when working with very large models that would otherwise require substantial computational resources.\\n\\n3. **Faster Training**: With fewer parameters to update, the training process becomes faster. This allows for quicker iterations and experimentation, which is valuable in research and development settings.\\n\\n4. **Better Generalization**: LoRA can help in achieving better generalization on downstream tasks. By focusing on low-rank adaptations, the model can avoid overfitting to the fine-tuning data, leading to improved performance on unseen data.\\n\\n5. **Scalability**: LoRA makes it feasible to fine-tune very large models that would be impractical to update fully. This scalability is crucial as models continue to grow in size and complexity.\\n\\n6. **Modularity**: The low-rank matrices introduced by LoRA can be seen as modular components that can be easily added or removed. This modularity allows for more flexible experimentation and adaptation of models to different tasks.\\n\\n7. **Cost-Effectiveness**: By reducing the computational and memory requirements, LoRA makes the fine-tuning process more cost-effective. This is particularly important for organizations with limited resources.\\n\\nOverall, LoRA provides a more efficient and scalable approach to fine-tuning large pre-trained models, making it a valuable technique in the field of machine learning.',\n",
       "   'reference': {'must_mention': ['reduce', 'parameters']}}},\n",
       " 'aggregate_metrics': None}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=agent_chain,\n",
    "    evaluation=eval_config,\n",
    "    verbose=True,\n",
    "    project_name=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:8]}\",\n",
    "    project_metadata={\"version\": \"1.0.0\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhTNe4kWrplB"
   },
   "source": [
    "## Part 2: LangGraph with Helpfulness:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1wKRddbIY_S"
   },
   "source": [
    "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
    "\n",
    "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
    "\n",
    "We're going to make a few key adjustments to account for this:\n",
    "\n",
    "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
    "2. We'll add a custom node and conditional edge to determine if the response was helpful enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npTYJ8ayR5B3"
   },
   "source": [
    "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "-LQ84YhyJG0w"
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "  messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gC8t-4FISCEh"
   },
   "source": [
    "We're going to add a custom helpfulness check here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ZV_PxI5zNY7f"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def check_helpfulness(state):\n",
    "  initial_query = state[\"messages\"][0]\n",
    "  final_response = state[\"messages\"][-1]\n",
    "\n",
    "  if len(state[\"messages\"]) > 10:\n",
    "    return \"END\"\n",
    "\n",
    "  prompt_template = \"\"\"\\\n",
    "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate \n",
    "  helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
    "\n",
    "  Initial Query:\n",
    "  {initial_query}\n",
    "\n",
    "  Final Response:\n",
    "  {final_response}\"\"\"\n",
    "\n",
    "  prompt_template = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "  helpfulness_chain = prompt_template | helpfulness_check_model | StrOutputParser()\n",
    "\n",
    "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
    "\n",
    "  if \"Y\" in helpfulness_response:\n",
    "    print(\"Helpful!\")\n",
    "    return \"end\"\n",
    "  else:\n",
    "    print(\"Not helpful!\")\n",
    "    return \"continue\"\n",
    "\n",
    "def dummy_node(state):\n",
    "  return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fz1u9Vf4SHxJ"
   },
   "source": [
    "####üèóÔ∏è Activity #4:\n",
    "\n",
    "Please write what is happening in our `check_helpfulness` function!\n",
    "\n",
    "---\n",
    "**ANSWER:**\n",
    "\n",
    "It checks if there are more than 10 messages, and if so, it stops the function to avoid an infinite (or very long) loop.\n",
    "\n",
    "Then, it uses the given prompt template to ask the LLM to evaluate a question/answer pair in a binary way (helpful / not helpful).\n",
    "\n",
    "It uses LangChain to pass the prompt template into the LLM, and parse the output as a simple string at the end.\n",
    "\n",
    "The query and the final responsee are evaluated using the method \"invoke\" of the chain.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sD7EV0HqSQcb"
   },
   "source": [
    "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oajBwLkFVi1N"
   },
   "source": [
    "####üèóÔ∏è Activity #5:\n",
    "\n",
    "Please write markdown for the following cells to explain what each is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6rN7feNVn9f"
   },
   "source": [
    "##### YOUR MARKDOWN HERE\n",
    "\n",
    "We create our LangGraph nodes: the agent, the tool and a passthrough node that doesn't do anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "6r6XXA5FJbVf"
   },
   "outputs": [],
   "source": [
    "graph_with_helpfulness_check = StateGraph(AgentState)\n",
    "\n",
    "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
    "graph_with_helpfulness_check.add_node(\"action\", call_tool)\n",
    "graph_with_helpfulness_check.add_node(\"passthrough\", dummy_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZ22o2mWVrfp"
   },
   "source": [
    "##### YOUR MARKDOWN HERE\n",
    "\n",
    "We set the \"agent\" node as the entry point for the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "HNWHwWxuRiLY"
   },
   "outputs": [],
   "source": [
    "graph_with_helpfulness_check.set_entry_point(\"agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BhnBW2YVsJO"
   },
   "source": [
    "##### YOUR MARKDOWN HERE\n",
    "\n",
    "We add a conditional edge leaving from the agent and dividing the flow between the action node and the passthrough node. The function \"should_continue\" selects which direction the flow will go.\n",
    "\n",
    "Then, we and another conditional edge leaving the passthrough and, using the function \"check_helpfulness\", we will be able to select between going to back to the agent to try again to get a better answer (if not helpful) or to the end (if helpful)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "aVTKnWMbP_8T"
   },
   "outputs": [],
   "source": [
    "graph_with_helpfulness_check.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\" : \"action\",\n",
    "        \"end\" : \"passthrough\"\n",
    "    }\n",
    ")\n",
    "\n",
    "graph_with_helpfulness_check.add_conditional_edges(\n",
    "    \"passthrough\",\n",
    "    check_helpfulness,\n",
    "    {\n",
    "        \"continue\" : \"agent\",\n",
    "        \"end\" : END\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGDLEWOIVtK0"
   },
   "source": [
    "##### YOUR MARKDOWN HERE\n",
    "\n",
    "We add an edge between the action to return the flow to the agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "cbDK2MbuREgU"
   },
   "outputs": [],
   "source": [
    "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSI8AOaEVvT-"
   },
   "source": [
    "##### YOUR MARKDOWN HERE\n",
    "\n",
    "Finally, we compile the graph to be able to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "oQldl8ERQ8lf"
   },
   "outputs": [],
   "source": [
    "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F67FGCMRVwGz"
   },
   "source": [
    "##### YOUR MARKDOWN HERE\n",
    "\n",
    "Here, we call the graph with the given input and print out the resulting messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B3oo8E-PRK1T",
    "outputId": "72096ca6-e78e-475a-dde5-fe076c5b776d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpful!\n",
      "Initial Query: Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\n",
      "\n",
      "\n",
      "Agent Response: ### LoRA (Low-Rank Adaptation)\n",
      "\n",
      "LoRA, or Low-Rank Adaptation, is a technique used in machine learning to fine-tune large pre-trained models efficiently. Instead of updating all the parameters of a model during fine-tuning, LoRA introduces a small number of additional trainable parameters that are low-rank matrices. These matrices are used to adapt the pre-trained model to new tasks with minimal computational overhead. This approach significantly reduces the number of parameters that need to be updated, making the fine-tuning process more efficient in terms of both time and memory.\n",
      "\n",
      "### Tim Dettmers\n",
      "\n",
      "Tim Dettmers is a researcher and expert in the field of machine learning, particularly known for his work on efficient training and inference of large neural networks. He has contributed to the development of techniques that make it feasible to train large models on consumer-grade hardware. His research often focuses on optimizing the computational aspects of deep learning, including methods like quantization, pruning, and low-rank adaptation.\n",
      "\n",
      "### Attention\n",
      "\n",
      "Attention is a mechanism in machine learning, particularly in the context of neural networks, that allows the model to focus on specific parts of the input data when making predictions. It was first introduced in the context of sequence-to-sequence models for tasks like machine translation. The attention mechanism assigns different weights to different parts of the input sequence, enabling the model to prioritize certain elements over others. This is particularly useful in handling long sequences where some parts of the input are more relevant to the task at hand than others.\n",
      "\n",
      "The most well-known application of attention is in the Transformer architecture, which has become the foundation for many state-of-the-art models in natural language processing, such as BERT and GPT. The attention mechanism in Transformers allows the model to consider the entire input sequence simultaneously, making it highly effective for tasks that require understanding the context and relationships between different parts of the input.\n",
      "\n",
      "Would you like more detailed information on any of these topics?\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
    "\n",
    "messages = agent_with_helpfulness_check.invoke(inputs)\n",
    "\n",
    "print_messages(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVmZPs6lnpsM"
   },
   "source": [
    "### Task 4: LangGraph for the \"Patterns\" of GenAI\n",
    "\n",
    "Let's ask our system about the 4 patterns of Generative AI:\n",
    "\n",
    "1. Prompt Engineering\n",
    "2. RAG\n",
    "3. Fine-tuning\n",
    "4. Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "ZoLl7GlXoae-"
   },
   "outputs": [],
   "source": [
    "patterns = [\"prompt engineering\", \"RAG\", \"fine-tuning\", \"LLM-based agents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zkh0YJuCp3Zl",
    "outputId": "1c29765b-42fc-449f-dce4-62dcb747a03b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpful!\n",
      "Initial Query: What is prompt engineering and when did it break onto the scene??\n",
      "\n",
      "\n",
      "Agent Response: Prompt engineering is a technique used primarily in the field of artificial intelligence (AI) and natural language processing (NLP). It involves crafting specific inputs (prompts) to guide the behavior and responses of AI models, particularly large language models like GPT-3, GPT-4, and others. The goal is to elicit desired outputs from the model by carefully designing the input text.\n",
      "\n",
      "### Key Aspects of Prompt Engineering:\n",
      "1. **Input Design**: Creating prompts that are clear, concise, and structured in a way that the AI can understand and respond to effectively.\n",
      "2. **Task Specification**: Defining the task or question in a manner that the AI can interpret correctly.\n",
      "3. **Context Provision**: Providing sufficient context within the prompt to help the AI generate relevant and accurate responses.\n",
      "4. **Iterative Refinement**: Continuously refining and testing prompts to improve the quality of the AI's output.\n",
      "\n",
      "### Historical Context:\n",
      "Prompt engineering became more prominent with the advent of large-scale language models like OpenAI's GPT-3, which was released in June 2020. These models demonstrated unprecedented capabilities in understanding and generating human-like text, making the design of effective prompts a critical skill for leveraging their full potential.\n",
      "\n",
      "### Timeline:\n",
      "- **Pre-2020**: Early AI and NLP models required more explicit programming and less flexible input methods.\n",
      "- **June 2020**: OpenAI released GPT-3, showcasing the power of large language models and bringing prompt engineering into the spotlight.\n",
      "- **Post-2020**: The field of prompt engineering has grown rapidly, with researchers and practitioners developing best practices, tools, and techniques to optimize AI interactions.\n",
      "\n",
      "Prompt engineering continues to evolve as AI models become more sophisticated, and it remains a crucial area of study and practice for anyone working with advanced NLP systems.\n",
      "\n",
      "\n",
      "\n",
      "Helpful!\n",
      "Initial Query: What is RAG and when did it break onto the scene??\n",
      "\n",
      "\n",
      "Agent Response: RAG stands for Retrieval-Augmented Generation. It is a framework that combines the strengths of retrieval-based and generation-based models to improve the performance of natural language processing tasks, particularly in the context of question answering and information retrieval.\n",
      "\n",
      "### Key Components of RAG:\n",
      "1. **Retriever**: This component is responsible for fetching relevant documents or passages from a large corpus based on the input query.\n",
      "2. **Generator**: This component generates a coherent and contextually appropriate response using the retrieved documents.\n",
      "\n",
      "### How RAG Works:\n",
      "1. **Query Input**: A user inputs a query.\n",
      "2. **Document Retrieval**: The retriever fetches relevant documents or passages from a pre-indexed corpus.\n",
      "3. **Response Generation**: The generator uses the retrieved documents to generate a response that is both relevant and contextually accurate.\n",
      "\n",
      "### Advantages of RAG:\n",
      "- **Improved Accuracy**: By leveraging external documents, RAG can provide more accurate and contextually relevant answers.\n",
      "- **Scalability**: It can handle large corpora of documents, making it suitable for applications requiring extensive knowledge bases.\n",
      "- **Flexibility**: It can be fine-tuned for specific domains or tasks, enhancing its versatility.\n",
      "\n",
      "### When Did RAG Break Onto the Scene?\n",
      "RAG was introduced by Facebook AI Research (FAIR) in a paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\" which was published in 2020. The paper demonstrated the effectiveness of RAG in various knowledge-intensive tasks, setting a new benchmark in the field of natural language processing.\n",
      "\n",
      "Would you like more detailed information or specific aspects of RAG?\n",
      "\n",
      "\n",
      "\n",
      "Helpful!\n",
      "Initial Query: What is fine-tuning and when did it break onto the scene??\n",
      "\n",
      "\n",
      "Agent Response: Fine-tuning is a process in machine learning where a pre-trained model is further trained on a new dataset to adapt it to a specific task. This approach leverages the knowledge the model has already acquired during its initial training on a large, general dataset, allowing it to perform well on a new, often smaller, and more specific dataset with relatively less training time and computational resources.\n",
      "\n",
      "### Key Steps in Fine-Tuning:\n",
      "1. **Pre-training**: A model is trained on a large, general dataset. For example, a language model might be trained on a vast corpus of text from the internet.\n",
      "2. **Fine-tuning**: The pre-trained model is then trained on a smaller, task-specific dataset. This step adjusts the model's parameters to better suit the specific task at hand.\n",
      "\n",
      "### Benefits of Fine-Tuning:\n",
      "- **Efficiency**: Reduces the amount of data and computational resources needed compared to training a model from scratch.\n",
      "- **Performance**: Often leads to better performance on specific tasks because the model starts with a strong foundation of general knowledge.\n",
      "\n",
      "### Historical Context:\n",
      "Fine-tuning became particularly prominent with the advent of deep learning and the development of large-scale pre-trained models. Some key milestones include:\n",
      "\n",
      "- **2014**: The concept of transfer learning, which underpins fine-tuning, gained traction with the success of models like AlexNet and VGG in computer vision tasks.\n",
      "- **2018**: The release of the BERT (Bidirectional Encoder Representations from Transformers) model by Google marked a significant breakthrough in natural language processing (NLP). BERT demonstrated the power of fine-tuning by achieving state-of-the-art results on a variety of NLP tasks.\n",
      "- **2019**: OpenAI's GPT-2 further showcased the potential of fine-tuning in generating coherent and contextually relevant text.\n",
      "\n",
      "Since then, fine-tuning has become a standard practice in the development of machine learning models, especially in fields like NLP, computer vision, and more recently, in multimodal models that handle text, images, and other types of data.\n",
      "\n",
      "\n",
      "\n",
      "Helpful!\n",
      "Initial Query: What is LLM-based agents and when did it break onto the scene??\n",
      "\n",
      "\n",
      "Agent Response: LLM-based agents, or Large Language Model-based agents, are artificial intelligence systems that leverage large language models to perform a variety of tasks. These tasks can include natural language understanding, text generation, translation, summarization, and more. The core technology behind these agents is typically a deep learning model trained on vast amounts of text data, enabling it to understand and generate human-like text.\n",
      "\n",
      "### Key Characteristics of LLM-based Agents:\n",
      "1. **Natural Language Understanding (NLU):** They can comprehend and interpret human language in a way that is contextually relevant.\n",
      "2. **Text Generation:** They can produce coherent and contextually appropriate text based on a given prompt.\n",
      "3. **Versatility:** They can be fine-tuned for specific tasks such as customer service, content creation, coding assistance, etc.\n",
      "4. **Scalability:** They can handle a wide range of queries and tasks, making them suitable for various applications.\n",
      "\n",
      "### Breakthrough and Evolution:\n",
      "The concept of LLM-based agents has been around for a while, but significant breakthroughs occurred in the late 2010s and early 2020s. Here are some key milestones:\n",
      "\n",
      "1. **2018 - BERT (Bidirectional Encoder Representations from Transformers):** Developed by Google, BERT was one of the first models to achieve state-of-the-art performance on a variety of natural language processing tasks by understanding the context of words in search queries.\n",
      "\n",
      "2. **2019 - GPT-2 (Generative Pre-trained Transformer 2):** OpenAI released GPT-2, which demonstrated the ability to generate coherent and contextually relevant text, sparking significant interest in the capabilities of large language models.\n",
      "\n",
      "3. **2020 - GPT-3 (Generative Pre-trained Transformer 3):** OpenAI's GPT-3, with 175 billion parameters, marked a significant leap in the capabilities of language models. It could perform a wide range of tasks with minimal fine-tuning, making it one of the most versatile and powerful language models at the time.\n",
      "\n",
      "4. **2021 and Beyond:** The development of even larger and more sophisticated models, such as Google's LaMDA and OpenAI's Codex, continued to push the boundaries of what LLM-based agents could achieve. These models have been integrated into various applications, from chatbots to coding assistants.\n",
      "\n",
      "### Applications:\n",
      "- **Customer Support:** Automated agents that can handle customer queries and provide support.\n",
      "- **Content Creation:** Tools that assist in writing articles, reports, and other forms of content.\n",
      "- **Programming Assistance:** Code generation and debugging tools.\n",
      "- **Education:** Personalized tutoring and educational content generation.\n",
      "- **Healthcare:** Assisting in medical documentation and providing information.\n",
      "\n",
      "The rapid advancements in LLM-based agents have made them a crucial component in various industries, driving innovation and efficiency.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pattern in patterns:\n",
    "  what_is_string = f\"What is {pattern} and when did it break onto the scene??\"\n",
    "  inputs = {\"messages\" : [HumanMessage(content=what_is_string)]}\n",
    "  messages = agent_with_helpfulness_check.invoke(inputs)\n",
    "  print_messages(messages)\n",
    "  print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
