{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fa_QpI0RXQKx"
   },
   "source": [
    "# LangSmith and Evaluation Overview with AI Makerspace\n",
    "\n",
    "Today we'll be looking at an amazing tool:\n",
    "\n",
    "[LangSmith](https://docs.smith.langchain.com/)!\n",
    "\n",
    "This tool will help us monitor, test, debug, and evaluate our LangChain applications - and more!\n",
    "\n",
    "We'll also be looking at a few Advanced Retrieval techniques along the way - and evaluate it using LangSmith!\n",
    "\n",
    "✋BREAKOUT ROOM #1:\n",
    "- Task 1: Dependencies and OpenAI API Key\n",
    "- Task 2: Basic RAG Chain\n",
    "- Task 3: Setting Up LangSmith\n",
    "- Task 4: Examining the Trace in LangSmith!\n",
    "- Task 5: Create Testing Dataset\n",
    "\n",
    "✋BREAKOUT ROOM #2:\n",
    "- Task 1: Parent Document Retriever\n",
    "- Task 2: Ensemble Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tw5ok9p-XuUs"
   },
   "source": [
    "## Task 1: Dependencies and OpenAI API Key\n",
    "\n",
    "We'll be using OpenAI's suite of models today to help us generate and embed our documents for a simple RAG system built on top of LangChain's blogs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jhSjB1O6-Y0J",
    "outputId": "9cc0c072-1117-4863-8010-ee37e8e33a3d"
   },
   "outputs": [],
   "source": [
    "!pip install langchain_core langchain_openai langchain_community langchain-qdrant qdrant-client langsmith openai tiktoken cohere -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ADl8-whIAUHD",
    "outputId": "a5794372-be42-46ee-cf7d-4e5628e97e9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "#import getpass\n",
    "\n",
    "#os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")\n",
    "\n",
    "# Dotenv library to load configuration from .env file\n",
    "import dotenv\n",
    "dotenvfile = dotenv.find_dotenv()\n",
    "PROJECT_ROOT = os.path.dirname(dotenvfile)  # Use dotenv to find root of the project\n",
    "\n",
    "# Load configuration from .env file\n",
    "dotenv.load_dotenv(dotenvfile)\n",
    "\n",
    "#print(os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_NpPwk1YAgl"
   },
   "source": [
    "## Task 2: Basic RAG Chain\n",
    "\n",
    "Now we'll set up our basic RAG chain, first up we need a model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUWXhsNVYLTA"
   },
   "source": [
    "### OpenAI Model\n",
    "\n",
    "\n",
    "We'll use OpenAI's `gpt-3.5-turbo` model to ensure we can use a stronger model for decent evaluation later!\n",
    "\n",
    "Notice that we can tag our resources - this will help us be able to keep track of which resources were used where later on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CSgK6jgw_tI3"
   },
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "base_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", tags=[\"base_llm\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiagvgVDYTPn"
   },
   "source": [
    "#### Asyncio Bug Handling\n",
    "\n",
    "This is necessary for Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ntIqnv4cA5gR"
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDO0XJqbYabb"
   },
   "source": [
    "### SiteMap Loader\n",
    "\n",
    "We'll use a SiteMapLoader to scrape the LangChain blogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sAS3QBQSARiw",
    "outputId": "c10b7cc4-e5c7-4dd8-c689-59d5c356c4d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "Fetching pages: 100%|##############################################################| 221/221 [00:20<00:00, 10.65it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import SitemapLoader\n",
    "\n",
    "documents = SitemapLoader(web_path=\"https://blog.langchain.dev/sitemap-posts.xml\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "_s_x87H0BYmn",
    "outputId": "2c74e338-8ba6-432f-c2da-641d5d55336e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://blog.langchain.dev/what-is-an-agent/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata[\"source\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F79PdFcaYfBL"
   },
   "source": [
    "### RecursiveCharacterTextSplitter\n",
    "\n",
    "We're going to use a relatively naive text splitting strategy today!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NmCdYTTTA4du"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "split_documents = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 20\n",
    ").split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yLA5-LNBBVM-",
    "outputId": "f5a05c71-c382-42f4-e6a5-58f3b61d66f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1548"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUsEc07iYnwj"
   },
   "source": [
    "### Embeddings\n",
    "\n",
    "We'll be leveraging OpenAI's [text-embedding-3-small](https://openai.com/index/new-embedding-models-and-api-updates/) today!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QVhMN0aaBrsM"
   },
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "base_embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLoO_2MaY0TS"
   },
   "source": [
    "### Qdrant VectorStore Retriever\n",
    "\n",
    "Now we can use a Qdrant VectorStore to embed and store our documents and then convert it to a retriever so it can be used in our chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "nBTK9kSFBWM1"
   },
   "outputs": [],
   "source": [
    "from langchain_qdrant import Qdrant\n",
    "\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    split_documents,\n",
    "    base_embeddings_model,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"langchainblogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZpwDxlniCJRu"
   },
   "outputs": [],
   "source": [
    "base_retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2GPhHPAY5yG"
   },
   "source": [
    "### Prompt Template\n",
    "\n",
    "All we have left is a prompt template, which we'll create here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "YAU74penCNmR"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "base_rag_prompt_template = \"\"\"\\\n",
    "Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "base_rag_prompt = ChatPromptTemplate.from_template(base_rag_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmT5VyLmZAAK"
   },
   "source": [
    "### LCEL Chain\n",
    "\n",
    "Now that we have:\n",
    "\n",
    "- Embeddings Model\n",
    "- Generation Model\n",
    "- Retriever\n",
    "- Prompt\n",
    "\n",
    "We're ready to build our LCEL chain!\n",
    "\n",
    "Keep in mind that we're returning our source documents with our queries - while this isn't necessary, it's a great thing to get into the habit of doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "pqVAsUc_Cp-7"
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "base_rag_chain = (\n",
    "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
    "    # \"question\" : populated by getting the value of the \"question\" key\n",
    "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
    "    {\"context\": itemgetter(\"question\") | base_retriever, \"question\": itemgetter(\"question\")}\n",
    "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
    "    #              by getting the value of the \"context\" key from the previous step\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
    "    #              into the LLM and stored in a key called \"response\"\n",
    "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
    "    | {\"response\": base_rag_prompt | base_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fNjMoS-ZVo5"
   },
   "source": [
    "Let's test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "6Dq9rCScDfBE",
    "outputId": "4bcebeb0-37ae-4fb8-9dae-ceba9cc1dc54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A good way to evaluate agents is by testing their capabilities in common agentic workflows, such as planning/task decomposition, function calling, and the ability to override pre-trained biases when needed. These tests can help determine an agent's effectiveness and generalizability for different tasks.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_rag_chain.invoke({\"question\" : \"What is a good way to evaluate agents?\"})[\"response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJtSdDsXZXam"
   },
   "source": [
    "## Task 3: Setting Up LangSmith\n",
    "\n",
    "Now that we have a chain - we're ready to get started with LangSmith!\n",
    "\n",
    "We're going to go ahead and use the following `env` variables to get our Colab notebook set up to start reporting.\n",
    "\n",
    "If all you needed was simple monitoring - this is all you would need to do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "iqPdBXSBD4a-"
   },
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "unique_id = uuid4().hex[0:8]\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"LangSmith - {unique_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith - 0c794ffb\n"
     ]
    }
   ],
   "source": [
    "print(os.environ[\"LANGCHAIN_PROJECT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ms4msyKLaIr6"
   },
   "source": [
    "### LangSmith API\n",
    "\n",
    "In order to use LangSmith - you will need a beta key, you can join the queue through the `Beta Sign Up` button on LangSmith's homepage!\n",
    "\n",
    "Join [here](https://www.langchain.com/langsmith)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MVq1EYngEMhV",
    "outputId": "587380f0-7395-4608-aa63-35d117dbd162"
   },
   "outputs": [],
   "source": [
    "#os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')\n",
    "#print(os.environ[\"LANGCHAIN_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qy0MMBLacXv"
   },
   "source": [
    "Let's test our our first generation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "3eoqBtBQERXP",
    "outputId": "727abc25-3510-49b7-9671-98406e672294"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangSmith is a unified platform for debugging, testing, evaluating, and monitoring LLM (Large Language Model) applications. It allows users to automate their feedback loop, improve iteration speed, collaborate, and organize workspaces effectively. LangSmith is now available for use and offers various features such as TypeScript improvements, open-source LangGraph framework, evaluation flows, and integration with Azure Marketplace for transactable offerings.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_rag_chain.invoke({\"question\" : \"What is LangSmith?\"}, {\"tags\" : [\"Demo Run\"]})['response']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZxABFzPr2ny"
   },
   "source": [
    "## Task 4: Examining the Trace in LangSmith!\n",
    "\n",
    "Head on over to your LangSmith web UI to check out how the trace looks in LangSmith!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c52o58AfsLK6"
   },
   "source": [
    "#### 🏗️ Activity #1:\n",
    "\n",
    "Include a screenshot of your trace and explain what it means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMzWpDK369i2"
   },
   "source": [
    "---\n",
    "**ANSWER:**\n",
    "\n",
    "![Langchain screenshot](aie3_week5day2.png)\n",
    "\n",
    "In this screenshot, we can see that the question invoked above (\"What is LangSmith?\") was sent over to Langsmith as a RunnableSequence. Then, all the runs inside that call are recorded and we can see the details of those calls, such as the input and output, the documents that were retrieved by our Retriever class, the response sent out by ChatOpenAI, how many tokens this request took and its cost, when it started and when it ended, how long it took, and so on. It is also interesting to see that our tag \"Demo Run\" has been assigned to this trace, making it easier for us to later distinguish between runs according to our needs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLxh0-thanXt"
   },
   "source": [
    "## Task 5: Create Testing Dataset\n",
    "\n",
    "Now we can create a dataset using some user defined questions, and providing the retrieved context as a \"ground truth\" context.\n",
    "\n",
    "> NOTE: There are many different ways you can approach this specific task - generating ground truth answers with AI, using human experts to generate golden datasets, and more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCKCAhASkXu0"
   },
   "source": [
    "### Synthetic Data Generation (SDG)\n",
    "\n",
    "In order to full test our RAG chain, and the various modifications we'll be using in the following notebook, we'll need to create a small synthetic dataset that is relevant to our task!\n",
    "\n",
    "Let's start by generating a series of questions - which begins with a simple model definition!\n",
    "\n",
    "> NOTE: We're going to be using a purposefully simplified datagen pipeline as an example today - but you could leverage the RAGAS SDG pipeline just as easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "-dgAkRzDlEgH"
   },
   "outputs": [],
   "source": [
    "question_model = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tv-HC4C8lWIS"
   },
   "source": [
    "Next up, we'll create some novel chunks from our source data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "2Sz7rw0-lhf8"
   },
   "outputs": [],
   "source": [
    "sdg_documents = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 20\n",
    ").split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pep-eqUbllrv"
   },
   "source": [
    "Now, let's ask some questions that could be answered from the provided chunks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "e75gwn4Tlt3w"
   },
   "outputs": [],
   "source": [
    "question_prompt_template = \"\"\"\\\n",
    "You are a University Professor creating questions for an exam. You must create a question for a given piece of context.\n",
    "\n",
    "The question must be answerable only using the provided context.\n",
    "\n",
    "Avoid creating questions that are ambiguous or vague. They should be specifically related to the context.\n",
    "\n",
    "Your output must only be the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "question_prompt = ChatPromptTemplate.from_template(question_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Pcb_R-G_odCl"
   },
   "outputs": [],
   "source": [
    "question_chain = question_prompt | question_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5YbGi7umtOB"
   },
   "source": [
    "Now we can loop through a subset of our context chunks and create question/context pairs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ot0HzAGBms90",
    "outputId": "0417f0df-ea17-4da0-dd3b-a6336b154c4c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 23/23 [00:22<00:00,  1.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "question_context_pairs = []\n",
    "\n",
    "for idx in tqdm(range(0, len(sdg_documents), 40)):\n",
    "  question = question_chain.invoke({\"context\" : sdg_documents[idx].page_content})\n",
    "  question_context_pairs.append({\"question\" : question.content, \"context\" : sdg_documents[idx].page_content, \"idx\" : idx})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fzhg3AJlmfW-",
    "outputId": "9c32bad2-f2db-471d-e526-ad430e427292"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the topic of the new series introduced by Harrison Chase on June 28, 2024?',\n",
       " 'context': 'What is an agent?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAll Posts\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nIn the Loop\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is an agent?\\nIntroducing a new series of musings on AI agents.\\n\\nHarrison Chase\\n4 min read\\nJun 28, 2024',\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_context_pairs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34zGDKtD9I0A"
   },
   "source": [
    "We'll repeat this process for answers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Ws624hGapJ92"
   },
   "outputs": [],
   "source": [
    "answer_prompt_template = \"\"\"\\\n",
    "You are a University Professor creating an exam. You must create a answer for a given piece of context and question.\n",
    "\n",
    "The answer must only rely on the provided context.\n",
    "\n",
    "Your output must only be the answer.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "answer_prompt = ChatPromptTemplate.from_template(answer_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "AEnETuSEqf2R"
   },
   "outputs": [],
   "source": [
    "answer_chain = answer_prompt | question_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89faPBcPqnfT",
    "outputId": "8e9bd52a-b400-44da-d228-34d9234c324e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 23/23 [00:33<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "for question_context_pair in tqdm(question_context_pairs):\n",
    "  question_context_pair[\"answer\"] = answer_chain.invoke({\"question\" : question_context_pair[\"question\"], \"context\" : question_context_pair[\"context\"]}).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "usB9PkJWq_0D",
    "outputId": "4cc1d45b-c389-4d18-a7cf-37e8efbdcf00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Question**: What is the topic of the new series introduced by Harrison Chase on June 28, 2024?**\n",
      "\n",
      "**Answer**: The topic of the new series introduced by Harrison Chase on June 28, 2024, is musings on AI agents.**\n",
      "\n",
      "**Context**: What is an agent? Skip to content All Posts Release Notes Case Studies In the Loop LangChain Docs Sign in Subscribe What is an agent? Introducing a new series of musings on AI agents. Harrison Chase 4 min read Jun 28, 2024**\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "**Question**: What are the four primary ways LangSmith helped the CommandBar team improve their Copilot user assistant?**\n",
      "\n",
      "**Answer**: Trace Visibility, Debugging, Increased testing coverage, and Monitoring.**\n",
      "\n",
      "**Context**: LangChain Partners with CommandBar on their Copilot User Assistant Skip to content All Posts Release Notes Case Studies In the Loop LangChain Docs Sign in Subscribe LangChain Partners with CommandBar on their Copilot User Assistant By LangChain 2 min read Feb 8, 2024 CommandBar is a user assistance platform that helps software companies make their products easy to use by capturing and predicting user intent, and then delivering personalized in-product help. CommandBar’s Copilot widget, which companies embed into their applications, goes beyond a typical chatbot. It can answer user questions, trigger personalized product tours, even fulfill a user’s intent directly by carrying out actions on their behalf.CommandBar’s customers have slightly different needs from each other, and in order for the Copilot to be helpful across all of CommandBar’s customers, the team had to find common threads for user assistance that could be streamlined or automated with LLMs as well integrate with many different content providers (such as help desks and knowledge bases) for information retrieval.CommandBar decided to use LangSmith to give them visibility over their Copilot’s performance and ultimately deliver better experiences for their customers. While the team did not use LangChain in production, getting up and running in LangSmith was fast. “I was surprised how straightforward it was to set up the traces just with the decorators in LangSmith. It was super easy to get started.” says Senior Software Engineer Jared Luxenberg. LangSmith helped the CommandBar team is these four ways primarily:Trace Visibility: The team was able to see if an end user had a bad experience just by looking at a LangSmith trace and didn’t have to rely on receiving a screenshot or email. LangSmith visibility down to the conversation thread allowed CommandBar to be proactive about identifying how the customer could avoid a bad interaction in the future.Debugging: Building a good Copilot came down to building a good retrieval system, and LangSmith traces helped the team understand if the right documents were even retrieved in the first place, and if not, they had information to try different techniques to improve the system.Increased testing coverage: The team 5x’d the number of tests it could run on any new code change. Before LangSmith, CommbandBar relied solely on manual QA, but after adopting LangSmith, they could augment human evaluation with better auto-evaluation over grounded pairs of question : response that were known to be good.Monitoring: The CommandBar team relied on LangSmith to alert if their LLM provider was having an outage, and they could keep a view of the overall health of their application in LangSmith’s monitoring tab.LangSmith mapped easily to the workflows they wanted to accomplish, aiding them throughout the entire application development lifecycle. CommandBar’s Copilot has been live since November 2023, and it makes life easier for thousands of support teams and millions of end users, and this translates into concrete results for customers – like a 44% decrease in support tickets from a recent case study “Every week we hear from one of customers enthusiastically sharing that the Copilot responses are so on point, and it’s become one of our product’s biggest competitive advantage and probably our flagship product at this point.” says Luxenberg. CommandBar believes Copilot can become even more useful and proactive for users. Commandbar has a lot of cool improvements rolling out in the coming months, which are all powered by Langsmith. To read more about their launch, head to their blog here. Tags By LangChainCase Studies Join our newsletter Updates from the LangChain team and community Enter your email Subscribe Processing your application... Success! Please check your inbox and click the link to confirm your subscription. Sorry, something went wrong. Please try again. You might also like Announcing LangGraph v0.1 & LangGraph Cloud: Running agents at scale, reliably By LangChain 6 min read Aligning LLM-as-a-Judge with Human Preferences By LangChain 5 min read How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x By LangChain 4 min read**\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "**Question**: What improvements were made in the LangChain v0.1.0 release earlier this year in January?**\n",
      "\n",
      "**Answer**: Since the release of langchain v0.1.0 earlier this year in January, improvements were made in the following areas:\n",
      "\n",
      "1. Standard Chat Model Interface: Standardized tool calling support and added a standardized interface for structuring output.\n",
      "2. Async Support: Improved async support for many core abstractions.\n",
      "3. Streaming Support: Added an Event Streaming API to improve streaming support.\n",
      "4. Partner Packages: Added dedicated packages for 20+ providers in Python and 17 providers in JavaScript.**\n",
      "\n",
      "**Context**: — but LangGraph is becoming the recommended way to build agents. We’ve added a prebuilt LangGraph object that is equivalent to AgentExecutor — which, by being built on LangGraph, is far easier to customize and modify. See here for documentation on how to migrate.Evolving v0.1.0: Improved support for streaming, standardized tool calling, and more Since the release of langchain v0.1.0 earlier this year in January, we’ve made sizable improvements in the following areas:Standard Chat Model Interface: We want to make it as easy as possible to switch seamlessly between different LLMs. In order to do this, we’ve standardized tool calling support as well as added a standardized interface for structuring output. Async Support: We’ve improved our async support for many core abstractions. Here’s an example or two. Huge thanks and shout out to @cbornet for helping make this a reality!Streaming Support: Streaming is crucial for LLM applications, and we’ve improved our streaming support by adding in an Event Streaming API.Partner Packages: Having stable and reliable integrations is a top priority for us. We’ve worked closely with ecosystem partners to add dedicated packages for 20+ providers in Python including MongoDB, Mistral, and Together AI – as well as 17 providers in JavaScript including Google VertexAI, Weaviate, and Cloudflare.How to upgradev0.2 contains many improvements, and we designed it to be largely backwards compatible with minimal breaking changes. We’ve also worked to add a migration CLI to ease any issues, as well as documentation highlighting what has changed between versions.Check out our GitHub Discussions thread for details on how to test the CLI and install the v0.2 pre-release. And stay tuned for a full migration guide on the week of May 20th. Why stability matters to usWe value the trust of our 1M+ developers relying on LangChain. As we evolve LangChain, we’re committed to delivering industry-leading solutions while ensuring a foundational framework for engineering teams to confidently use in production. While langchain and langchain-core are currently in a pre-1.0 state, we strive to minimize breaking changes and deprecate classes at least 1 full breaking release ahead of time (3-6 months). Our release cadence also ensures regular updates and bug fixes, keeping the LangChain platform reliable and production-ready. We’ll also continue to maintain a 0.1 version, to which we’ll push critical bug fixes for 3 months. See here for more on our release and deprecation policy.We’d love to hear from you on GitHub on all things LangChain v2.0. And if you’re new to LangChain, follow our quickstart guide to get started.**\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pair in question_context_pairs[0:3]:\n",
    "    q = pair['question']\n",
    "    a = pair['answer']\n",
    "    c = ' '.join(pair['context'].split())\n",
    "    print(f'**Question**: {q}**\\n')\n",
    "    print(f'**Answer**: {a}**\\n')\n",
    "    print(f'**Context**: {c}**\\n')\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyBtPs7p9Mbz"
   },
   "source": [
    "Now we can set up our LangSmith client - and we'll add the above created dataset to our LangSmith instance!\n",
    "\n",
    "> NOTE: Read more about this process [here](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#create-from-list-of-values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "T9exE2e6F3gF"
   },
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "dataset_name = \"langsmith-demo-dataset-v3\"\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name, description=\"LangChain Blog Test Questions\"\n",
    ")\n",
    "\n",
    "for triplet in question_context_pairs:\n",
    "  client.create_example(\n",
    "      inputs={\"question\" : triplet[\"question\"]},\n",
    "      outputs={\"answer\" : triplet[\"answer\"]},\n",
    "      dataset_id=dataset.id\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXgi14vSbFIc"
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "Now we can run the evaluation!\n",
    "\n",
    "We'll need to start by preparing some custom data preparation functions to ensure our chain works with the expected inputs/outputs from the `evaluate` process in LangSmith.\n",
    "\n",
    "> NOTE: More reading on this available [here](https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-a-langchain-runnable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "fbjnv3bMwQKg"
   },
   "outputs": [],
   "source": [
    "def prepare_data_ref(run, example):\n",
    "  return {\n",
    "      \"prediction\" : run.outputs[\"response\"],\n",
    "      \"reference\" : example.outputs[\"answer\"],\n",
    "      \"input\" : example.inputs[\"question\"]\n",
    "  }\n",
    "\n",
    "def prepare_data_noref(run, example):\n",
    "  return {\n",
    "      \"prediction\" : run.outputs[\"response\"],\n",
    "      \"input\" : example.inputs[\"question\"]\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OuwnMdtl9nwz"
   },
   "source": [
    "We'll be using a few custom evaluators to evaluate our pipeline, as well as a few \"built in\" methods!\n",
    "\n",
    "Check out the built-ins [here](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "65d1b263094a4c6abdaf8688285c41cc",
      "087e4c759a35424385156c76e3780144",
      "18ffc7ae91d1408c8747f8d0a972f3a8",
      "8cbac09f7ba540bd8f79dde47e49fe5f",
      "b5e2be01c1834c78b7ce0cbe1f7a9d09",
      "ef2e833ac73548ee837f906d4b004a3d",
      "2c984bd9861c4ec0b27f0c3a2020dede",
      "c9927b2052be4016b0feb88ab583d32e",
      "6fbddf6663954ff48ee0cb0b4a202acb",
      "e6035d68dd9e4c6598ac5eae2d0deac1",
      "b6b1f2be05a34e118dc55869149b60ac"
     ]
    },
    "id": "CENtd4K_IQa3",
    "outputId": "528f629a-abf9-4f9b-9af4-3756aa40cc50"
   },
   "outputs": [],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "eval_llm = ChatOpenAI(temperature=0.0, \n",
    "                      model=\"gpt-4o\",\n",
    "                      max_tokens=None,\n",
    "                      timeout=120,\n",
    "                      max_retries=2,\n",
    "                     )\n",
    "\n",
    "cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\", prepare_data=prepare_data_ref, config={\"llm\": eval_llm})\n",
    "\n",
    "unlabeled_dopeness_evaluator = LangChainStringEvaluator(\n",
    "    \"criteria\",\n",
    "    config={\n",
    "        \"criteria\" : {\n",
    "            \"dopeness\" : \"Is the answer to the question dope, meaning cool - awesome - and legit?\"\n",
    "        },\n",
    "        \"llm\": eval_llm,\n",
    "    },\n",
    "    prepare_data=prepare_data_noref\n",
    ")\n",
    "\n",
    "labeled_score_evaluator = LangChainStringEvaluator(\n",
    "    \"labeled_score_string\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"accuracy\": \"Is the generated answer the same as the reference answer?\"\n",
    "        },\n",
    "        \"llm\": eval_llm\n",
    "    },\n",
    "    prepare_data=prepare_data_ref\n",
    ")\n",
    "\n",
    "unlabeled_coherence_evaluator = LangChainStringEvaluator(\"criteria\", \n",
    "                                                         config={\n",
    "                                                             \"criteria\": \"coherence\",\n",
    "                                                             \"llm\": eval_llm\n",
    "                                                         }, \n",
    "                                                         prepare_data=prepare_data_noref,\n",
    "                                                        )\n",
    "labeled_relevance_evaluator = LangChainStringEvaluator(\"labeled_criteria\", \n",
    "                                                       config={ \n",
    "                                                           \"criteria\": \"relevance\",\n",
    "                                                           \"llm\": eval_llm\n",
    "                                                        }, \n",
    "                                                       prepare_data=prepare_data_ref\n",
    "                                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "65d1b263094a4c6abdaf8688285c41cc",
      "087e4c759a35424385156c76e3780144",
      "18ffc7ae91d1408c8747f8d0a972f3a8",
      "8cbac09f7ba540bd8f79dde47e49fe5f",
      "b5e2be01c1834c78b7ce0cbe1f7a9d09",
      "ef2e833ac73548ee837f906d4b004a3d",
      "2c984bd9861c4ec0b27f0c3a2020dede",
      "c9927b2052be4016b0feb88ab583d32e",
      "6fbddf6663954ff48ee0cb0b4a202acb",
      "e6035d68dd9e4c6598ac5eae2d0deac1",
      "b6b1f2be05a34e118dc55869149b60ac"
     ]
    },
    "id": "CENtd4K_IQa3",
    "outputId": "528f629a-abf9-4f9b-9af4-3756aa40cc50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Base RAG Evaluation v5-59e58ea2' at:\n",
      "https://smith.langchain.com/o/fcaff44a-5186-52ee-9403-69aa238eb874/datasets/27b3b04a-dedb-4146-94c9-e9ab0c1ce024/compare?selectedSessions=f6e194ba-585e-44a5-95a3-f7e09d46925b\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b48b0002e34b5d80f593a2ca806e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_rag_results = evaluate(\n",
    "    base_rag_chain.invoke,\n",
    "    data=dataset_name,\n",
    "    evaluators=[\n",
    "        cot_qa_evaluator,\n",
    "        #unlabeled_dopeness_evaluator,\n",
    "        #labeled_score_evaluator,\n",
    "        #unlabeled_coherence_evaluator,  ## Removing some evaluators to avoid rate limits\n",
    "        #labeled_relevance_evaluator\n",
    "        ],\n",
    "    experiment_prefix=\"Base RAG Evaluation v5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwhBxlxYAdno"
   },
   "source": [
    "## Testing Other Retrievers\n",
    "\n",
    "Now we can test our how changing our Retriever impacts our LangSmith evaluation!\n",
    "\n",
    "We'll build this simple qa_chain factory to create standardized qa_chains where the only different component will be the retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "qnfy4VNkzZi2"
   },
   "outputs": [],
   "source": [
    "def create_qa_chain(retriever):\n",
    "  primary_qa_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "  created_qa_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever,\n",
    "     \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | RunnablePassthrough.assign(\n",
    "        context=itemgetter(\"context\")\n",
    "      )\n",
    "    | {\n",
    "         \"response\": base_rag_prompt | primary_qa_llm,\n",
    "         \"context\": itemgetter(\"context\"),\n",
    "      }\n",
    "  )\n",
    "  return created_qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOPp4Xq7AvEx"
   },
   "source": [
    "### Task 1: Parent Document Retriever\n",
    "\n",
    "One of the easier ways we can imagine improving a retriever is to embed our documents into small chunks, and then retrieve a significant amount of additional context that \"surrounds\" the found context.\n",
    "\n",
    "You can read more about this method [here](https://python.langchain.com/docs/modules/data_connection/retrievers/parent_document_retriever)!\n",
    "\n",
    "The basic outline of this retrieval method is as follows:\n",
    "\n",
    "1. Obtain User Question\n",
    "2. Retrieve child documents using Dense Vector Retrieval\n",
    "3. Merge the child documents based on their parents. If they have the same parents - they become merged.\n",
    "4. Replace the child documents with their respective parent documents from an in-memory-store.\n",
    "5. Use the parent documents to augment generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "67I6QJAJ0Un7"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "client = QdrantClient(\":memory:\")\n",
    "client.create_collection(\n",
    "    collection_name=\"split_parents\",\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "vectorstore = Qdrant(client, collection_name=\"split_parents\", embeddings=base_embeddings_model)\n",
    "\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "zfk5RYUt00Pw"
   },
   "outputs": [],
   "source": [
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=RecursiveCharacterTextSplitter(chunk_size=400),\n",
    "    parent_splitter=RecursiveCharacterTextSplitter(chunk_size=2000),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "68c1t4o104AK"
   },
   "outputs": [],
   "source": [
    "parent_document_retriever.add_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTH0MDolBndm"
   },
   "source": [
    "Let's create, test, and then evaluate our new chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "KMjLfqOC09Iw"
   },
   "outputs": [],
   "source": [
    "parent_document_retriever_qa_chain = create_qa_chain(parent_document_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "Rv8bAHPN1H4P",
    "outputId": "46487575-438d-4c12-d306-7a4341d7ed83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG stands for Retrieval Augmented Generation, which is a central paradigm in LLM (Large Language Models) application development. It involves connecting LLMs to external data sources to retrieve relevant documents based on a user query and generate an answer grounded in the retrieved context.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_document_retriever_qa_chain.invoke({\"question\" : \"What is RAG?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6H7vHCt2HJi"
   },
   "source": [
    "#### Evaluating the Parent Document Retrieval Pipeline\n",
    "\n",
    "Now that we've created a new retriever - let's try evaluating it on the same dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "142e5756a6c840aeb25b733ece015ed2",
      "4777a17d7d9c444a8d44e070faef0ad0",
      "dad645f3259c41b5a50e0e94906ca3e3",
      "c67428191d074e4d80cb0f28fb65cd41",
      "8db8082c5ab741b5b4fd78027e65135d",
      "2ab8452345f24b46b172515ddc77fdb0",
      "d5ad942ac5ed41c6a581b4022f177114",
      "27e2bff47a084ed6bc7d96ac09af9ebc",
      "d097ee02256d40d5b395d37cd0face05",
      "13c75ca5ee9d42eb8476a307f30b7528",
      "e8143f8ceee6468c98433734ff59b835"
     ]
    },
    "id": "Z-0WFCtx2N4n",
    "outputId": "341b1af3-cd74-46a9-d9d1-5ab2190ec96c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Parent Document Retrieval RAG Evaluation-cf7725ad' at:\n",
      "https://smith.langchain.com/o/fcaff44a-5186-52ee-9403-69aa238eb874/datasets/27b3b04a-dedb-4146-94c9-e9ab0c1ce024/compare?selectedSessions=4f958dbb-bf3e-4685-8e5d-a8089d7efde2\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150b0219b4f44af7b46eec69d6e38866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pdr_rag_results = evaluate(\n",
    "    parent_document_retriever_qa_chain.invoke,\n",
    "    data=dataset_name,\n",
    "    evaluators=[\n",
    "        cot_qa_evaluator,\n",
    "        #unlabeled_dopeness_evaluator,\n",
    "        #labeled_score_evaluator,\n",
    "        #unlabeled_coherence_evaluator,\n",
    "        #labeled_relevance_evaluator\n",
    "        ],\n",
    "    experiment_prefix=\"Parent Document Retrieval RAG Evaluation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JaNk6o7_BqX8"
   },
   "source": [
    "### Task 2: Ensemble Retrieval\n",
    "\n",
    "Next let's look at ensemble retrieval!\n",
    "\n",
    "You can read more about this [here](https://python.langchain.com/docs/modules/data_connection/retrievers/ensemble)!\n",
    "\n",
    "The basic idea is as follows:\n",
    "\n",
    "1. Obtain User Question\n",
    "2. Hit the Retriever Pair\n",
    "    - Retrieve Documents with BM25 Sparse Vector Retrieval\n",
    "    - Retrieve Documents with Dense Vector Retrieval Method\n",
    "3. Collect and \"fuse\" the retrieved docs based on their weighting using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm into a single ranked list.\n",
    "4. Use those documents to augment our generation.\n",
    "\n",
    "Ensure your `weights` list - the relative weighting of each retriever - sums to 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "zz7dl1GD5-L-"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Vs8wxT9b5pRA"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=450, chunk_overlap=75)\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(split_documents)\n",
    "bm25_retriever.k = 2\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectorstore = Qdrant.from_documents(split_documents, embedding, location=\":memory:\")\n",
    "qdrant_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[\n",
    "        bm25_retriever,\n",
    "        qdrant_retriever\n",
    "    ],\n",
    "    weights=[\n",
    "        0.5,\n",
    "        0.5\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "cv69YDpF6PrJ"
   },
   "outputs": [],
   "source": [
    "ensemble_retriever_qa_chain = create_qa_chain(ensemble_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "6lSszzrf6UmP",
    "outputId": "ea13ffbc-df0f-4191-f873-6c2f0405d874"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG stands for Retrieval Augmented Generation, which is a central paradigm in LLM (Large Language Models) application development. It involves connecting LLMs to external data sources to address the lack of recent or private information during training.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_retriever_qa_chain.invoke({\"question\" : \"What is RAG?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "e99b5317081e42c199a297f3a483bbf5",
      "42bfd25ca7a14d629e4070b093a197d5",
      "fa9f9fc79b344271adc94acc487aef59",
      "a603cf6ec7d544a1a951fb8defccb976",
      "7448f6f5d9dc431fba689d8821285ca3",
      "1d0589c63fd1461b93b403c594b6ccfa",
      "5e500672dafc4529806e4e5f72fe97fa",
      "ece363bdc8b54631beba2628e3175e0b",
      "46a7a454e9bb486588ac33156abddecc",
      "f79f87ac933b45fa9e2c17871d4f2e44",
      "a888bb92387549fea8843f20e3b4c50f"
     ]
    },
    "id": "GVBY5lhm4KG7",
    "outputId": "d48d2604-1ac3-4b8a-c4aa-93214b4c0e06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Hybrid Retrieval RAG Evaluation-c3b39ad8' at:\n",
      "https://smith.langchain.com/o/fcaff44a-5186-52ee-9403-69aa238eb874/datasets/27b3b04a-dedb-4146-94c9-e9ab0c1ce024/compare?selectedSessions=95343171-3113-4398-8c18-3f06b0b8e6da\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0350a32e931649fc8e4eb43a1dbc14b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pdr_rag_results = evaluate(\n",
    "    ensemble_retriever_qa_chain.invoke,\n",
    "    data=dataset_name,\n",
    "    evaluators=[\n",
    "        cot_qa_evaluator,\n",
    "        #unlabeled_dopeness_evaluator,\n",
    "        #labeled_score_evaluator,\n",
    "        #unlabeled_coherence_evaluator,\n",
    "        #labeled_relevance_evaluator\n",
    "        ],\n",
    "    experiment_prefix=\"Hybrid Retrieval RAG Evaluation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPocfrNFiYWi"
   },
   "source": [
    "#### ❓Question #1:\n",
    "\n",
    "What conclusions can you draw about the above results?\n",
    "\n",
    "Describe in your own words what the metrics are expressing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**ANSWER:**\n",
    "\n",
    "Unfortunately, I could not run all the evaluators, as I kept running into rate limit errors. I edited my code so that I ran only the `cot contextual accuracy` evaluator. \n",
    "\n",
    "From my own results (in screenshot below), I see that Baseline actually gave the best Contextual Accuracy, followed by Parent-Document retrieval, and finally the Hybrid retrieval as last. This is in contrast to the results that we see in class, in which, Contextual accuracy is equivalent for both Parent-Document and Hybrid Retrieval, and it drops significantly for the baseline retrieval. \n",
    "\n",
    "![Experiment Results](aie3_week5day2_2.png)\n",
    "\n",
    "As I don't have my own results from the other evaluators, I will discuss the results we saw in class. Parent-Document Retrieval gives better Coherence than both Hybrid Retrieval and Baseline. \"Dopeness\" is significantly worse for the Hybrid Retrieval, and best for the Parent-Document retrieval. Relevance is the same for all three retrieval options.\n",
    "\n",
    "*Meaning of the metrics:*\n",
    "\n",
    "\n",
    "- **cot_qa_evaluator** is the off-the-shelf LangSmith evaluator \"Chain of Thought Q&A\". In this, the LLMis asked to reference \"context\" in determining correctness, and the prompt instructs the LLM to use chain of thought \"reasoning\" before determining a final verdict.\n",
    "- **dopeness**: Is our custom evaluator that uses a criteria defined by ourselves, in which we ask the LLM to judge whether \"the answer to the question dope, meaning cool - awesome - and legit?\"\n",
    "- **labeled_score_evaluator**: another custom evaluator that evaluates for what we called \"accuracy\", meaning \"Is the generated answer the same as the reference answer?\". In this, we give the evaluator the right answer (the \"label\") and ask it to compare the response to it.\n",
    "- **unlabeled_coherence_evaluator**: This evaluator uses a pre-defined criteria called \"coherence\", in a scale from 0 to 1.\n",
    "- **labeled_relevance_evaluator**: This evaluator uses a pre-defined criteria called \"relevance\", in a scale from 0 to 1, comparing the answer to the given label.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "087e4c759a35424385156c76e3780144": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ef2e833ac73548ee837f906d4b004a3d",
      "placeholder": "​",
      "style": "IPY_MODEL_2c984bd9861c4ec0b27f0c3a2020dede",
      "value": ""
     }
    },
    "13c75ca5ee9d42eb8476a307f30b7528": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "142e5756a6c840aeb25b733ece015ed2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4777a17d7d9c444a8d44e070faef0ad0",
       "IPY_MODEL_dad645f3259c41b5a50e0e94906ca3e3",
       "IPY_MODEL_c67428191d074e4d80cb0f28fb65cd41"
      ],
      "layout": "IPY_MODEL_8db8082c5ab741b5b4fd78027e65135d"
     }
    },
    "18ffc7ae91d1408c8747f8d0a972f3a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c9927b2052be4016b0feb88ab583d32e",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6fbddf6663954ff48ee0cb0b4a202acb",
      "value": 1
     }
    },
    "1d0589c63fd1461b93b403c594b6ccfa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "27e2bff47a084ed6bc7d96ac09af9ebc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "2ab8452345f24b46b172515ddc77fdb0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c984bd9861c4ec0b27f0c3a2020dede": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "42bfd25ca7a14d629e4070b093a197d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d0589c63fd1461b93b403c594b6ccfa",
      "placeholder": "​",
      "style": "IPY_MODEL_5e500672dafc4529806e4e5f72fe97fa",
      "value": ""
     }
    },
    "46a7a454e9bb486588ac33156abddecc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4777a17d7d9c444a8d44e070faef0ad0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ab8452345f24b46b172515ddc77fdb0",
      "placeholder": "​",
      "style": "IPY_MODEL_d5ad942ac5ed41c6a581b4022f177114",
      "value": ""
     }
    },
    "5e500672dafc4529806e4e5f72fe97fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "65d1b263094a4c6abdaf8688285c41cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_087e4c759a35424385156c76e3780144",
       "IPY_MODEL_18ffc7ae91d1408c8747f8d0a972f3a8",
       "IPY_MODEL_8cbac09f7ba540bd8f79dde47e49fe5f"
      ],
      "layout": "IPY_MODEL_b5e2be01c1834c78b7ce0cbe1f7a9d09"
     }
    },
    "6fbddf6663954ff48ee0cb0b4a202acb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7448f6f5d9dc431fba689d8821285ca3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8cbac09f7ba540bd8f79dde47e49fe5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6035d68dd9e4c6598ac5eae2d0deac1",
      "placeholder": "​",
      "style": "IPY_MODEL_b6b1f2be05a34e118dc55869149b60ac",
      "value": " 12/? [01:07&lt;00:00,  2.29s/it]"
     }
    },
    "8db8082c5ab741b5b4fd78027e65135d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a603cf6ec7d544a1a951fb8defccb976": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f79f87ac933b45fa9e2c17871d4f2e44",
      "placeholder": "​",
      "style": "IPY_MODEL_a888bb92387549fea8843f20e3b4c50f",
      "value": " 22/? [01:19&lt;00:00,  2.61s/it]"
     }
    },
    "a888bb92387549fea8843f20e3b4c50f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b5e2be01c1834c78b7ce0cbe1f7a9d09": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6b1f2be05a34e118dc55869149b60ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c67428191d074e4d80cb0f28fb65cd41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13c75ca5ee9d42eb8476a307f30b7528",
      "placeholder": "​",
      "style": "IPY_MODEL_e8143f8ceee6468c98433734ff59b835",
      "value": " 22/? [01:16&lt;00:00,  2.24s/it]"
     }
    },
    "c9927b2052be4016b0feb88ab583d32e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "d097ee02256d40d5b395d37cd0face05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d5ad942ac5ed41c6a581b4022f177114": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dad645f3259c41b5a50e0e94906ca3e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27e2bff47a084ed6bc7d96ac09af9ebc",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d097ee02256d40d5b395d37cd0face05",
      "value": 1
     }
    },
    "e6035d68dd9e4c6598ac5eae2d0deac1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8143f8ceee6468c98433734ff59b835": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e99b5317081e42c199a297f3a483bbf5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_42bfd25ca7a14d629e4070b093a197d5",
       "IPY_MODEL_fa9f9fc79b344271adc94acc487aef59",
       "IPY_MODEL_a603cf6ec7d544a1a951fb8defccb976"
      ],
      "layout": "IPY_MODEL_7448f6f5d9dc431fba689d8821285ca3"
     }
    },
    "ece363bdc8b54631beba2628e3175e0b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "ef2e833ac73548ee837f906d4b004a3d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f79f87ac933b45fa9e2c17871d4f2e44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa9f9fc79b344271adc94acc487aef59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ece363bdc8b54631beba2628e3175e0b",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_46a7a454e9bb486588ac33156abddecc",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
