{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wa8ykQk92aLX"
   },
   "source": [
    "# Evaluation of RAG Using Ragas\n",
    "\n",
    "In the following notebook we'll explore how to evaluate RAG pipelines using a powerful open-source tool called \"Ragas\". This will give us tools to evaluate component-wise metrics, as well as end-to-end metrics about the performance of our RAG pipelines.\n",
    "\n",
    "In the following notebook we'll complete the following tasks:\n",
    "\n",
    "- 🤝 Breakout Room Part #1:\n",
    "  1. Install required libraries\n",
    "  2. Set Environment Variables\n",
    "  3. Creating a simple RAG pipeline with [LangChain v0.2.0](https://python.langchain.com/v0.2/docs/versions/v0_2/)\n",
    "  4. Synthetic Dataset Generation for Evaluation using the [Ragas](https://github.com/explodinggradients/ragas) framework.\n",
    "  \n",
    "\n",
    "- 🤝 Breakout Room Part #2:\n",
    "  1. Evaluating our pipeline with Ragas\n",
    "  3. Making Adjustments to our RAG Pipeline\n",
    "  4. Evaluating our Adjusted pipeline against our baseline\n",
    "  5. Testing OpenAI's Claim\n",
    "\n",
    "The only way to get started is to get started - so let's grab our dependencies for the day!\n",
    "\n",
    "> NOTE: Using this notebook as presented will occur a charge of ~$3USD from OpenAI usage. Most of this cost is produced by the Synthetic Data Generation step - if you want to reduce costs, please use the provided commented code to leverage `GPT-3.5-Turbo` as the `critic_llm`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8h4yh6f7q9uN"
   },
   "source": [
    "## Motivation\n",
    "\n",
    "A claim, made by OpenAI, is that their `text-embedding-3-small` is better (generally) than their `text-embedding-ada-002` model.\n",
    "\n",
    "Here's some passages from their [blog](https://openai.com/blog/new-embedding-models-and-api-updates) about the `text-embedding-3` release:\n",
    "\n",
    "> `text-embedding-3-small` is our new highly efficient embedding model and provides a significant upgrade over its predecessor, the `text-embedding-ada-002` model...\n",
    "\n",
    "> **Stronger performance.** Comparing `text-embedding-ada-002` to `text-embedding-3-small`, the average score on a commonly used benchmark for multi-language retrieval ([MIRACL](https://github.com/project-miracl/miracl)) has increased from 31.4% to 44.0%, while the average score on a commonly used benchmark for English tasks ([MTEB](https://github.com/embeddings-benchmark/mteb)) has increased from 61.0% to 62.3%.\n",
    "\n",
    "Well, with a library like Ragas - we can put that claim to the test!\n",
    "\n",
    "If what they claim is true - we should see an increase on related metrics by using the new embedding model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAH1znJ2pIp3"
   },
   "source": [
    "# 🤝 Breakout Room Part #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpkXAmMZpLhm"
   },
   "source": [
    "## Task 1: Installing Required Libraries\n",
    "\n",
    "A reminder that one of the [key features](https://blog.langchain.dev/langchain-v0-1-0/) of LangChain v0.1.0 is the compartmentalization of the various LangChain ecosystem packages!\n",
    "\n",
    "So let's begin grabbing all of our LangChain related packages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5BN13TZlSCv4",
    "outputId": "51d9c154-af83-42b2-ce72-9656729ecb9d"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q langchain langchain-openai langchain_core langchain-community langchainhub openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fm7gXsD6pqG0"
   },
   "source": [
    "We'll also get the \"star of the show\" today, which is Ragas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zvAvDNWBpjQ1",
    "outputId": "20ff8c89-11db-4071-b0a0-6b9bfc0e215f"
   },
   "outputs": [],
   "source": [
    "!pip install -qU ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9q6Z9oTpw3X"
   },
   "source": [
    "We'll be leveraging [QDrant](https://qdrant.tech/) again as our LangChain `VectorStore`.\n",
    "\n",
    "We'll also install `pymupdf` and its dependencies which will allow us to load PDFs using the `PyMuPDFLoader` in the `langchain-community` package!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sAJK95napn8I",
    "outputId": "58f04109-385b-44c7-d3cb-4547d8acaea1"
   },
   "outputs": [],
   "source": [
    "!pip install -qU qdrant-client pymupdf pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_C2JvG1qO3h"
   },
   "source": [
    "## Task 2: Set Environment Variables\n",
    "\n",
    "Let's set up our OpenAI API key so we can leverage their API later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Lhqp5rUThG-",
    "outputId": "97cb739d-66b4-4476-ca04-b6257004178f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "# import getpass\n",
    "\n",
    "# Dotenv library to load configuration from .env file\n",
    "import dotenv\n",
    "dotenvfile = dotenv.find_dotenv()\n",
    "PROJECT_ROOT = os.path.dirname(dotenvfile)  # Use dotenv to find root of the project\n",
    "\n",
    "# Load configuration from .env file\n",
    "dotenv.load_dotenv(dotenvfile)\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "#os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key\")\n",
    "#print(os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFbWNvo3rZ4H"
   },
   "source": [
    "## Task 3: Creating a Simple RAG Pipeline with LangChain v0.1.0\n",
    "\n",
    "Building on what we learned last week, we'll be leveraging LangChain v0.1.0 and LCEL to build a simple RAG pipeline that we can baseline with Ragas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DV_BOewX8CW0"
   },
   "source": [
    "## Building our RAG pipeline\n",
    "\n",
    "Let's review the basic steps of RAG again:\n",
    "\n",
    "- Create an Index\n",
    "- Use retrieval to obtain pieces of context from our Index that are similar to our query\n",
    "- Use a LLM to generate responses based on the retrieved context\n",
    "\n",
    "Let's get started by creating our index.\n",
    "\n",
    "> NOTE: We're going to start leaning on the term \"index\" to refer to our `VectorStore`, `VectorDatabase`, etc. We can think of \"index\" as the catch-all term, whereas `VectorStore` and the like relate to the specific technologies used to create, store, and interact with the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1VDGJdxCJEVc"
   },
   "source": [
    "### Creating an Index\n",
    "\n",
    "You'll notice that the largest changes (outside of some import changes) are that our old favourite chains are back to being bundled in an easily usable abstraction.\n",
    "\n",
    "We can still create custom chains using LCEL - but we can also be more confident that our pre-packaged chains are creating using LCEL under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmFFThawK8lO"
   },
   "source": [
    "#### Loading Data\n",
    "\n",
    "Let's start by loading some data!\n",
    "\n",
    "> NOTE: You'll notice that we're using a document loader from the community package of LangChain. This is part of the v0.2.0 changes that make the base (`langchain-core`) package remain lightweight while still providing access to some of the more powerful community integrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DTDNFXaBSO2j"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(\n",
    "    \"https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf\",\n",
    ")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i3dJYlBCIX_p",
    "outputId": "1383c5b7-bb72-49ea-d323-fcd9eed48d60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf',\n",
       " 'file_path': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf',\n",
       " 'page': 0,\n",
       " 'total_pages': 195,\n",
       " 'format': 'PDF 1.3',\n",
       " 'title': 'The Pmarca Blog Archives',\n",
       " 'author': '',\n",
       " 'subject': '',\n",
       " 'keywords': '',\n",
       " 'creator': '',\n",
       " 'producer': 'Mac OS X 10.10 Quartz PDFContext',\n",
       " 'creationDate': \"D:20150110020418Z00'00'\",\n",
       " 'modDate': \"D:20150110020418Z00'00'\",\n",
       " 'trapped': ''}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQUl3sbZK4_1"
   },
   "source": [
    "#### Transforming Data\n",
    "\n",
    "Now that we've got our single document - let's split it into smaller pieces so we can more effectively leverage it with our retrieval chain!\n",
    "\n",
    "We'll start with the classic: `RecursiveCharacterTextSplitter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6Nt2E1xnLNgr"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap = 50\n",
    ")\n",
    "\n",
    "documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilzwQxhiLcVV"
   },
   "source": [
    "Let's confirm we've split our document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4wRw6a4aLfWh",
    "outputId": "a707bbf6-6338-45fc-a75e-86d693dfe2c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1864"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZ93HkYcMJwW"
   },
   "source": [
    "#### Loading OpenAI Embeddings Model\n",
    "\n",
    "We'll need a process by which we can convert our text into vectors that allow us to compare to our query vector.\n",
    "\n",
    "Let's use OpenAI's `text-embedding-ada-002` for this task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JU6CrDVZMgKe"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVtZR9JPLtR4"
   },
   "source": [
    "#### Creating a QDrant VectorStore\n",
    "\n",
    "Now that we have documents - we'll need a place to store them alongside their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "978TWiCtMA0B"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "qdrant_vector_store = Qdrant.from_documents(\n",
    "    documents,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"PMarca Blogs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vk50NmrMDlWu"
   },
   "source": [
    "####❓ Question #1:\n",
    "\n",
    "List out a few of the techniques that Qdrant uses that make it performant.\n",
    "\n",
    "> NOTE: Check the [documentation](https://qdrant.tech/documentation/overview/) for more information about QDrant!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**ANSWER:**\n",
    "\n",
    "Qdrant, along with other vector databases, are made for storing and querying high-dimensional vectors efficiently, using specialized data structures and indexes. These databases enable fast similarity and semantic search while allowing users to find vectors that are the closest to a given query vector based on some distance metric. \n",
    "\n",
    "Qdrant, specifically, uses:\n",
    "\n",
    "- Hierarchical Navigable Small World (HNSW), which is used to implement Approximate Nearest Neighbors. HNSW is a graph-based indexing algorithm, which builds a multi-layer navigation structure according to certain rules. The upper layers are more sparse and the distances between nodes are farther. The lower layers are denser and the distances between nodes are closer. The search starts from the uppermost layer, finds the node closest to the target in this layer, and then enters the next layer to begin another search. This is repeated until the target is approached.\n",
    "- Compression techniques such as Product Quantization, Scalar Quantization and Binary Quantization, which enable efficient storage and search of high-dimensional vectors by transforming the original vectors into new representations with less precision with minimal loss in precision.\n",
    "\n",
    "In addition, Qdrant has a few other features that make it a good choice for a Vector database, such as:\n",
    "- Multi-Vector Support: Qdrant allows the integration of multiple vectors per document. This enables more nuanced and relevant search results by combining different aspects of the data, such as the title and body of a document.\n",
    "- Option to store vectors in memory or on-disk, allowin gthe user to decide on the trade-off of speed vs storage capacity\n",
    "- Distributed deployment, to distribute the data across the peers to extend the storage capabilities and increase stability.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7ht6bJX9PAY"
   },
   "source": [
    "#### Creating a Retriever\n",
    "\n",
    "To complete our index, all that's left to do is expose our vectorstore as a retriever - which we can do the same way we would in previous version of LangChain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xne8P5dQTUiR"
   },
   "outputs": [],
   "source": [
    "retriever = qdrant_vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sO_DFBVKNvNm"
   },
   "source": [
    "#### Testing our Retriever\n",
    "\n",
    "Now that we've gone through the trouble of creating our retriever - let's see it in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "I9_ONxpnN0n6"
   },
   "outputs": [],
   "source": [
    "retrieved_documents = retriever.invoke(\"What is a rule of thumb for selecting an industry to invest in?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Za12yt4OBy1",
    "outputId": "6dfa1ae5-8198-49a1-b213-96b34a1a5147"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='the existing order — and make sure that those forces of change\\nhave a reasonable chance at succeeding.\\nSecond rule of thumb:\\nOnce you have picked an industry, get right to the center of it' metadata={'source': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'file_path': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'page': 125, 'total_pages': 195, 'format': 'PDF 1.3', 'title': 'The Pmarca Blog Archives', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'Mac OS X 10.10 Quartz PDFContext', 'creationDate': \"D:20150110020418Z00'00'\", 'modDate': \"D:20150110020418Z00'00'\", 'trapped': '', '_id': '28cf674d6adc4fbebebb238d471b649d', '_collection_name': 'PMarca Blogs'}\n",
      "page_content='Third rule:\\nIn a rapidly changing Held like technology, the best place to\\nget experience when you’re starting out is in younger, high-\\ngrowth companies.' metadata={'source': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'file_path': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'page': 127, 'total_pages': 195, 'format': 'PDF 1.3', 'title': 'The Pmarca Blog Archives', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'Mac OS X 10.10 Quartz PDFContext', 'creationDate': \"D:20150110020418Z00'00'\", 'modDate': \"D:20150110020418Z00'00'\", 'trapped': '', '_id': '2477b3fa0af54150b25f876c72176c85', '_collection_name': 'PMarca Blogs'}\n",
      "page_content='where the great opportunities can be found.\\nApply this rule when selecting which company to go to. Go to\\nthe company where all the action is happening.' metadata={'source': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'file_path': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'page': 125, 'total_pages': 195, 'format': 'PDF 1.3', 'title': 'The Pmarca Blog Archives', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'Mac OS X 10.10 Quartz PDFContext', 'creationDate': \"D:20150110020418Z00'00'\", 'modDate': \"D:20150110020418Z00'00'\", 'trapped': '', '_id': '58f9cd5f57ec4d958af493cbf3f6a0e4', '_collection_name': 'PMarca Blogs'}\n",
      "page_content='growth companies.\\n(This is not necessarily true in older and more established\\nindustries, but those aren’t the industries we’re talking about.)' metadata={'source': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'file_path': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'page': 127, 'total_pages': 195, 'format': 'PDF 1.3', 'title': 'The Pmarca Blog Archives', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'Mac OS X 10.10 Quartz PDFContext', 'creationDate': \"D:20150110020418Z00'00'\", 'modDate': \"D:20150110020418Z00'00'\", 'trapped': '', '_id': 'ee55adcb7eea4b469d4b5d52b4fd2c43', '_collection_name': 'PMarca Blogs'}\n"
     ]
    }
   ],
   "source": [
    "for doc in retrieved_documents:\n",
    "  print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8MKsT6JTgCU"
   },
   "source": [
    "### Creating a RAG Chain\n",
    "\n",
    "Now that we have the \"R\" in RAG taken care of - let's look at creating the \"AG\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zs7qBLaEQEic"
   },
   "source": [
    "#### Creating a Prompt Template\n",
    "\n",
    "There are a few different ways we could create our prompt template - we could create a custom template, as seen in the code below, or we could simply pull a prompt from the prompt hub! Let's look at an example of that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "eRCq_OKUQbKk"
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "retrieval_qa_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FziTftV5Q1H-",
    "outputId": "21189f0e-4b5d-4146-8071-eb0fff4a6f13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer any use questions based solely on the context below:\n",
      "\n",
      "<context>\n",
      "{context}\n",
      "</context>\n"
     ]
    }
   ],
   "source": [
    "print(retrieval_qa_prompt.messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyq88IPFRGoT"
   },
   "source": [
    "As you can see - the prompt template is simple (and has a small error) - so we'll create our own to be a bit more specific!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ijSNkTAjTsep"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context. \n",
    "If you cannot answer the question with the context, please respond with 'I don't know':\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYHnPaXl-cvJ"
   },
   "source": [
    "#### Setting Up our Basic QA Chain\n",
    "\n",
    "Now we can instantiate our basic RAG chain!\n",
    "\n",
    "We'll use LCEL directly just to see an example of it - but you could just as easily use an abstraction here to achieve the same goal!\n",
    "\n",
    "We'll also ensure to pass-through our context - which is critical for RAGAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "-TsjUWjbUfbW"
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "primary_qa_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "retrieval_augmented_qa_chain = (\n",
    "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
    "    # \"question\" : populated by getting the value of the \"question\" key\n",
    "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
    "    #              by getting the value of the \"context\" key from the previous step\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
    "    #              into the LLM and stored in a key called \"response\"\n",
    "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
    "    | {\"response\": prompt | primary_qa_llm, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MgAa9JwBuJx"
   },
   "source": [
    "####🏗️ Activity #1:\n",
    "\n",
    "Describe the pipeline shown above in simple terms. You can include a diagram if desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**ANSWER:**\n",
    "\n",
    "In short, the pipeline takes the question asked by the user and runs it through the vector store retrieval process to get the top-n most similar documents to the question as the context. Both the question and the context (as a dictionary) are passed forward to the prompt template, which formats the prompt, and then forwarded to the primary QA LLM (in this case gpt-3.5-turbo), so that it returns a response. The context is also provided in the final dictionary, unchanged.\n",
    "\n",
    "A step-by-step description of the pipeline is this:\n",
    "\n",
    "The pipeline starts with creating a dictionary with two keys, \"context\" and \"question\". The question is simply the same string informed by the user, which is forwarded to the chain via the `invoke` method. The \"context\" key, on the other hand, is created by passing the original question through the \"retriever\" function, which has been previously defined as a Qdrand vector store retriever. This will search in the vector store for the most similar documents to the question and assign it to the \"context\" key. These steps happen in parallel. \n",
    "\n",
    "Next, the RunnablePassthrough step allows the question to be passed forward unchanged, and the context gets assigned to the dictionary as well.\n",
    "\n",
    "Finally, both keys (context and question) are passed to the promt template for proper formatting and then to the LLM, which will generate the response. The result will be a dictionary with the response from the LLM and the retrieved context.\n",
    "  \n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zO69de-F-oMD"
   },
   "source": [
    "Let's test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2FS5NxC6UyU2",
    "outputId": "db9953a2-758d-4723-cd95-e980a47715d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get right to the center of it.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is a rule of thumb for selecting an industry to invest in?\"\n",
    "\n",
    "result = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
    "\n",
    "print(result[\"response\"].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tIuHVGPOO9P2",
    "outputId": "24ce3524-9284-4eea-f78b-4329a615d321"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n",
      "[Document(page_content='ask if you can call them again if things change.\\nTrust me — they’d much rather be saying “yes” than “no” —\\nthey need all the good investments they can get.\\nSecond, consider the environment.', metadata={'source': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'file_path': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'page': 15, 'total_pages': 195, 'format': 'PDF 1.3', 'title': 'The Pmarca Blog Archives', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'Mac OS X 10.10 Quartz PDFContext', 'creationDate': \"D:20150110020418Z00'00'\", 'modDate': \"D:20150110020418Z00'00'\", 'trapped': '', '_id': '6e6f1b1ac1894673bf8ec36952c6d058', '_collection_name': 'PMarca Blogs'}), Document(page_content='watching carefully — if everyone agrees right up front that\\nwhatever you are doing makes total sense, it probably isn’t a new\\nand radical enough idea to justify a new company.', metadata={'source': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'file_path': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'page': 152, 'total_pages': 195, 'format': 'PDF 1.3', 'title': 'The Pmarca Blog Archives', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'Mac OS X 10.10 Quartz PDFContext', 'creationDate': \"D:20150110020418Z00'00'\", 'modDate': \"D:20150110020418Z00'00'\", 'trapped': '', '_id': 'f52e28e31bdd42078c23ba553b5c4f7c', '_collection_name': 'PMarca Blogs'}), Document(page_content='Third rule:\\nIn a rapidly changing Held like technology, the best place to\\nget experience when you’re starting out is in younger, high-\\ngrowth companies.', metadata={'source': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'file_path': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'page': 127, 'total_pages': 195, 'format': 'PDF 1.3', 'title': 'The Pmarca Blog Archives', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'Mac OS X 10.10 Quartz PDFContext', 'creationDate': \"D:20150110020418Z00'00'\", 'modDate': \"D:20150110020418Z00'00'\", 'trapped': '', '_id': '2477b3fa0af54150b25f876c72176c85', '_collection_name': 'PMarca Blogs'}), Document(page_content='the existing order — and make sure that those forces of change\\nhave a reasonable chance at succeeding.\\nSecond rule of thumb:\\nOnce you have picked an industry, get right to the center of it', metadata={'source': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'file_path': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'page': 125, 'total_pages': 195, 'format': 'PDF 1.3', 'title': 'The Pmarca Blog Archives', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'Mac OS X 10.10 Quartz PDFContext', 'creationDate': \"D:20150110020418Z00'00'\", 'modDate': \"D:20150110020418Z00'00'\", 'trapped': '', '_id': '28cf674d6adc4fbebebb238d471b649d', '_collection_name': 'PMarca Blogs'})]\n"
     ]
    }
   ],
   "source": [
    "question = \"What did Pink Floyd have to say about how to proceed when investing in a new industry?\"\n",
    "\n",
    "result = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
    "\n",
    "print(result[\"response\"].content)\n",
    "print(result[\"context\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-XYZueEP42k"
   },
   "source": [
    "We can already see that there are some improvements we could make here.\n",
    "\n",
    "For now, let's switch gears to RAGAS to see how we can leverage that tool to provide us insight into how our pipeline is performing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOECHyzHRqDw"
   },
   "source": [
    "## Task 4: Synthetic Dataset Generation for Evaluation using Ragas\n",
    "\n",
    "Ragas is a powerful library that lets us evaluate our RAG pipeline by collecting input/output/context triplets and obtaining metrics relating to a number of different aspects of our RAG pipeline.\n",
    "\n",
    "We'll be evaluating on every core metric today, but in order to do that - we'll need to create a test set. Luckily for us, Ragas can do that directly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqXQ0jweWJOu"
   },
   "source": [
    "### Synthetic Test Set Generation\n",
    "\n",
    "We can leverage Ragas' [`Synthetic Test Data generation`](https://docs.ragas.io/en/stable/concepts/testset_generation.html) functionality to generate our own synthetic QC pairs - as well as a synthetic ground truth - quite easily!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "nVk5SlU9znXe"
   },
   "outputs": [],
   "source": [
    "loader = PyMuPDFLoader(\n",
    "    \"https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf\",\n",
    ")\n",
    "\n",
    "eval_documents = loader.load()\n",
    "\n",
    "text_splitter_eval = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 600,\n",
    "    chunk_overlap = 50\n",
    ")\n",
    "\n",
    "eval_documents = text_splitter_eval.split_documents(eval_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7rOQkxhzrq3"
   },
   "source": [
    "####❓ Question #2:\n",
    "\n",
    "Why is it important to split our documents using different parameters when creating our synthetic data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**ANSWER:**\n",
    "\n",
    "This is important because if the document chunks are exactly the same as the original chunks used for setting up the RAG pipeline, we risk having synthetic questions that will end up not evaluating our RAG properly. This is because, as mentioned in the Ragas documentation, \"LLMs by default are not good at creating diverse samples as it tends to follow common paths\". If our evaluation questions are exactly the same as the documents in the RAG pipeline, we will end up not being able to create diverse and hard enough questions that will test the RAG pipeline in conditions that would be most similar to a production environment. It will be something akin to overfitting, as our evaluation will only show us how well the RAG pipeline works with questions extracted from the exact same environment in which the RAG was trained.\n",
    "\n",
    "(review)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hiAPYw-hz-zo",
    "outputId": "fc8c6829-5c53-4eb1-8407-1545f5a7d023"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "624"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYCrVMW9Blda"
   },
   "source": [
    "\n",
    "> NOTE: 🛑 Using this notebook as presented will occur a charge of ~$3USD from OpenAI usage. Most of this cost is produced by the Synthetic Data Generation step - if you want to reduce costs, please use the provided commented code to leverage GPT-3.5-Turbo as the critic_llm. If you're attempting to create a lot of samples please be aware of cost, as well as rate limits. 🛑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "f75fdd56268a4b83a7fb7e4a3b2cce82",
      "1fb5a4b71deb406fa2f342c88b9e4e1d",
      "37ec9b5c847749439d7c155ac3b1ec68",
      "1317f4e20e1c4574a360345b427c3e8a",
      "2aa53858803d4ad39113009d86dd67fc",
      "7e1d22c19aff4c768d643c249e425d00",
      "3a498872a68049329b4d206629b9b3bf",
      "89a7c333d0b241169dc29ed998b2c9c4",
      "88c8557741734e59a6099bb5fa260f6e",
      "92ef10fab64c4f40a93da3d31b572016",
      "3b43c3f561e34d019007ac9a0125b28d",
      "05ab48866b5d49df9567ce9cbda5ee2e",
      "49c1ef316e404052a7c8528781db3f9a",
      "2dcb3e2fdf164e35a27a79cfae65933a",
      "202f4244384a4501bfc1ffa50af96a1f",
      "d93698b0506743ff98fdb998cfb7080a",
      "19acd28bfa2e4a7a83bc42faea5de770",
      "356b929fa8dc42538767c58dcce12217",
      "60a663f8736a43bcb47ac6c5f37ec597",
      "e6edc46811064de2b74a6a477c4a44b7",
      "a10a7577a99b4683a1d59a09d88f93a1",
      "444bc7dae1aa4e098b79655428599310"
     ]
    },
    "id": "IXc6sMglSej_",
    "outputId": "1d4904f0-9674-448a-9af3-f99da62cc8f3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes:   0%|          | 0/1248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filename and doc_id are the same for all nodes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24dd275fafe40dab2e20e6f06666176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the four major forms of chance and th...</td>\n",
       "      <td>[that chance is immune from human intervention...</td>\n",
       "      <td>nan</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does the concept of commitment relate to o...</td>\n",
       "      <td>[you can reply to a lot of messages with “I’m ...</td>\n",
       "      <td>The concept of commitment relates to organizin...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are some criteria to consider when evalua...</td>\n",
       "      <td>[How to hire the best people you've\\never work...</td>\n",
       "      <td>There are many aspects to hiring great people,...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why does the author believe that intelligence ...</td>\n",
       "      <td>[Criteria 7rst\\nLots of people will tell you t...</td>\n",
       "      <td>The author believes that intelligence is overr...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What aspects of the startup culture in the Val...</td>\n",
       "      <td>[would have spent 10 or 20 or 30 years buildin...</td>\n",
       "      <td>The culture of startups in the Valley contribu...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What are the potential consequences of not bei...</td>\n",
       "      <td>[Here’s why you shouldn’t do that:\\nWhat are t...</td>\n",
       "      <td>The potential consequences of not being able t...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How did cheap bandwidth contribute to the grow...</td>\n",
       "      <td>[late 90’s — due to commodity hardware, open s...</td>\n",
       "      <td>Cheap bandwidth in the late 90's contributed t...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the significance of a large market in ...</td>\n",
       "      <td>[billion people online now. That is a very lar...</td>\n",
       "      <td>A large market in the context of the internet ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How can a transformative deal inject energy ba...</td>\n",
       "      <td>[cut some big deals, do some spinoWs, whatever...</td>\n",
       "      <td>nan</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How does living a structured existence affect ...</td>\n",
       "      <td>[Arnold Schwarzenegger’s success as a movie st...</td>\n",
       "      <td>Living a structured existence can limit one's ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How does perceived risk impact decision-making...</td>\n",
       "      <td>[sonal relationships within the group — so the...</td>\n",
       "      <td>The desire to be liked in a startup with multi...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What is the author's perspective on the signif...</td>\n",
       "      <td>[relation between raw intelligence, as measure...</td>\n",
       "      <td>The author believes that intelligence is signi...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What task can showcase the skills of an outdat...</td>\n",
       "      <td>[who, at their core, can’t program.\\nAnd it’s ...</td>\n",
       "      <td>nan</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What are the consequences and trade-offs of th...</td>\n",
       "      <td>[from that era that are doing well today versu...</td>\n",
       "      <td>The consequences and trade-offs of the money r...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What's a simple and effective way to minimize ...</td>\n",
       "      <td>[Answer the Xrst one when it rings, but never ...</td>\n",
       "      <td>One of the best and easiest ways to avoid dist...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What is the purpose of using temporary subfold...</td>\n",
       "      <td>[you can reply to a lot of messages with “I’m ...</td>\n",
       "      <td>The purpose of using temporary subfolders in e...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What factors determine the peak age of creativ...</td>\n",
       "      <td>[ods when productivity is highest, the peak ag...</td>\n",
       "      <td>The peak age of creative impact is determined ...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What evidence beyond mainstream Internet busin...</td>\n",
       "      <td>[billion people online now. That is a very lar...</td>\n",
       "      <td>nan</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   What are the four major forms of chance and th...   \n",
       "1   How does the concept of commitment relate to o...   \n",
       "2   What are some criteria to consider when evalua...   \n",
       "3   Why does the author believe that intelligence ...   \n",
       "4   What aspects of the startup culture in the Val...   \n",
       "5   What are the potential consequences of not bei...   \n",
       "6   How did cheap bandwidth contribute to the grow...   \n",
       "7   What is the significance of a large market in ...   \n",
       "8   How can a transformative deal inject energy ba...   \n",
       "9   How does living a structured existence affect ...   \n",
       "10  How does perceived risk impact decision-making...   \n",
       "11  What is the author's perspective on the signif...   \n",
       "12  What task can showcase the skills of an outdat...   \n",
       "13  What are the consequences and trade-offs of th...   \n",
       "14  What's a simple and effective way to minimize ...   \n",
       "15  What is the purpose of using temporary subfold...   \n",
       "16  What factors determine the peak age of creativ...   \n",
       "17  What evidence beyond mainstream Internet busin...   \n",
       "\n",
       "                                             contexts  \\\n",
       "0   [that chance is immune from human intervention...   \n",
       "1   [you can reply to a lot of messages with “I’m ...   \n",
       "2   [How to hire the best people you've\\never work...   \n",
       "3   [Criteria 7rst\\nLots of people will tell you t...   \n",
       "4   [would have spent 10 or 20 or 30 years buildin...   \n",
       "5   [Here’s why you shouldn’t do that:\\nWhat are t...   \n",
       "6   [late 90’s — due to commodity hardware, open s...   \n",
       "7   [billion people online now. That is a very lar...   \n",
       "8   [cut some big deals, do some spinoWs, whatever...   \n",
       "9   [Arnold Schwarzenegger’s success as a movie st...   \n",
       "10  [sonal relationships within the group — so the...   \n",
       "11  [relation between raw intelligence, as measure...   \n",
       "12  [who, at their core, can’t program.\\nAnd it’s ...   \n",
       "13  [from that era that are doing well today versu...   \n",
       "14  [Answer the Xrst one when it rings, but never ...   \n",
       "15  [you can reply to a lot of messages with “I’m ...   \n",
       "16  [ods when productivity is highest, the peak ag...   \n",
       "17  [billion people online now. That is a very lar...   \n",
       "\n",
       "                                         ground_truth evolution_type  \\\n",
       "0                                                 nan         simple   \n",
       "1   The concept of commitment relates to organizin...         simple   \n",
       "2   There are many aspects to hiring great people,...         simple   \n",
       "3   The author believes that intelligence is overr...         simple   \n",
       "4   The culture of startups in the Valley contribu...         simple   \n",
       "5   The potential consequences of not being able t...         simple   \n",
       "6   Cheap bandwidth in the late 90's contributed t...         simple   \n",
       "7   A large market in the context of the internet ...         simple   \n",
       "8                                                 nan         simple   \n",
       "9   Living a structured existence can limit one's ...         simple   \n",
       "10  The desire to be liked in a startup with multi...  multi_context   \n",
       "11  The author believes that intelligence is signi...  multi_context   \n",
       "12                                                nan  multi_context   \n",
       "13  The consequences and trade-offs of the money r...  multi_context   \n",
       "14  One of the best and easiest ways to avoid dist...  multi_context   \n",
       "15  The purpose of using temporary subfolders in e...  multi_context   \n",
       "16  The peak age of creative impact is determined ...  multi_context   \n",
       "17                                                nan      reasoning   \n",
       "\n",
       "                                             metadata  episode_done  \n",
       "0   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "1   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "2   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "3   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "4   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "5   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "6   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "7   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "8   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "9   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "10  [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "11  [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "12  [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "13  [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "14  [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "15  [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "16  [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "17  [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "critic_llm = ChatOpenAI(model=\"gpt-3.5-turbo\") #<--- If you don't have GPT-4 access, or to reduce cost/rate limiting issues.\n",
    "#critic_llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "distributions = {\n",
    "    simple: 0.5,\n",
    "    multi_context: 0.4,\n",
    "    reasoning: 0.1\n",
    "}\n",
    "\n",
    "testset = generator.generate_with_langchain_docs(eval_documents, 20, distributions, is_async = False)\n",
    "testset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOIGT0XLz8ze"
   },
   "source": [
    "####❓ Question #3:\n",
    "\n",
    "`{simple: 0.5, reasoning: 0.25, multi_context: 0.25}`\n",
    "\n",
    "What exactly does this mapping refer to?\n",
    "\n",
    "> NOTE: Check out the Ragas documentation on this generation process [here](https://docs.ragas.io/en/stable/concepts/testset_generation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**ANSWER:**\n",
    "\n",
    "This mapping shows our desired proportion of the different types of questions to be generated by the algorithm. Currently, Ragas has the following types of \"evolutions\" that generate these different types of questions: \"Simple\", \"Reasoning\", \"Conditioning\" and \"Multi-Context\".\n",
    "\n",
    "- Simple: Simple questions.\n",
    "- Reasoning: Rewrite the question in a way that enhances the need for reasoning to answer it effectively.\n",
    "- Conditioning: Modify the question to introduce a conditional element, which adds complexity to the question.\n",
    "- Multi-Context: Rephrase the question in a manner that necessitates information from multiple related sections or chunks to formulate an answer.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MemL406rUzBu"
   },
   "source": [
    "Let's look at the output and see what we can learn about it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RaCDdImVU15s",
    "outputId": "31efbb94-f09d-4d50-8c6e-59202aaeb5c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataRow(question='What are the four major forms of chance and their reasons in human interactions and creative reactions?', contexts=['that chance is immune from human interventions. However, one\\nmust be careful not to read any unconsciously purposeful intent\\ninto “interventions”… [which] are to be viewed as accidental,\\nunwilled, inadvertent, and unforseeable.\\nIndeed, chance plays several distinct roles when humans react cre-\\natively with one another and with their environment…\\nWe can observe chance arriving in four major forms and for four\\ndiWerent reasons. The principles involved aWect everyone.\\nHere’s where it helps to be a neurologist writing on this topic:\\nThe four kinds of chance each have a diWerent kind of motor'], ground_truth='nan', evolution_type='simple', metadata=[{'source': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'file_path': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'page': 167, 'total_pages': 195, 'format': 'PDF 1.3', 'title': 'The Pmarca Blog Archives', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'Mac OS X 10.10 Quartz PDFContext', 'creationDate': \"D:20150110020418Z00'00'\", 'modDate': \"D:20150110020418Z00'00'\", 'trapped': ''}])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.test_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrPsVwUAWFWB"
   },
   "source": [
    "### Generating Responses with RAG Pipeline\n",
    "\n",
    "Now that we have some QC pairs, and some ground truths, let's evaluate our RAG pipeline using Ragas.\n",
    "\n",
    "The process is, again, quite straightforward - thanks to Ragas and LangChain!\n",
    "\n",
    "Let's start by extracting our questions and ground truths from our create testset.\n",
    "\n",
    "We can start by converting our test dataset into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "frvzu1YxX8kY"
   },
   "outputs": [],
   "source": [
    "test_df = testset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GFKMIY8IZU8m",
    "outputId": "ed137f4f-df2c-41fa-d868-802d30076ea0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the four major forms of chance and th...</td>\n",
       "      <td>[that chance is immune from human intervention...</td>\n",
       "      <td>nan</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does the concept of commitment relate to o...</td>\n",
       "      <td>[you can reply to a lot of messages with “I’m ...</td>\n",
       "      <td>The concept of commitment relates to organizin...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are some criteria to consider when evalua...</td>\n",
       "      <td>[How to hire the best people you've\\never work...</td>\n",
       "      <td>There are many aspects to hiring great people,...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why does the author believe that intelligence ...</td>\n",
       "      <td>[Criteria 7rst\\nLots of people will tell you t...</td>\n",
       "      <td>The author believes that intelligence is overr...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What aspects of the startup culture in the Val...</td>\n",
       "      <td>[would have spent 10 or 20 or 30 years buildin...</td>\n",
       "      <td>The culture of startups in the Valley contribu...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What are the potential consequences of not bei...</td>\n",
       "      <td>[Here’s why you shouldn’t do that:\\nWhat are t...</td>\n",
       "      <td>The potential consequences of not being able t...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How did cheap bandwidth contribute to the grow...</td>\n",
       "      <td>[late 90’s — due to commodity hardware, open s...</td>\n",
       "      <td>Cheap bandwidth in the late 90's contributed t...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the significance of a large market in ...</td>\n",
       "      <td>[billion people online now. That is a very lar...</td>\n",
       "      <td>A large market in the context of the internet ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How can a transformative deal inject energy ba...</td>\n",
       "      <td>[cut some big deals, do some spinoWs, whatever...</td>\n",
       "      <td>nan</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How does living a structured existence affect ...</td>\n",
       "      <td>[Arnold Schwarzenegger’s success as a movie st...</td>\n",
       "      <td>Living a structured existence can limit one's ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How does perceived risk impact decision-making...</td>\n",
       "      <td>[sonal relationships within the group — so the...</td>\n",
       "      <td>The desire to be liked in a startup with multi...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What is the author's perspective on the signif...</td>\n",
       "      <td>[relation between raw intelligence, as measure...</td>\n",
       "      <td>The author believes that intelligence is signi...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What task can showcase the skills of an outdat...</td>\n",
       "      <td>[who, at their core, can’t program.\\nAnd it’s ...</td>\n",
       "      <td>nan</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What are the consequences and trade-offs of th...</td>\n",
       "      <td>[from that era that are doing well today versu...</td>\n",
       "      <td>The consequences and trade-offs of the money r...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What's a simple and effective way to minimize ...</td>\n",
       "      <td>[Answer the Xrst one when it rings, but never ...</td>\n",
       "      <td>One of the best and easiest ways to avoid dist...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What is the purpose of using temporary subfold...</td>\n",
       "      <td>[you can reply to a lot of messages with “I’m ...</td>\n",
       "      <td>The purpose of using temporary subfolders in e...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What factors determine the peak age of creativ...</td>\n",
       "      <td>[ods when productivity is highest, the peak ag...</td>\n",
       "      <td>The peak age of creative impact is determined ...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What evidence beyond mainstream Internet busin...</td>\n",
       "      <td>[billion people online now. That is a very lar...</td>\n",
       "      <td>nan</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   What are the four major forms of chance and th...   \n",
       "1   How does the concept of commitment relate to o...   \n",
       "2   What are some criteria to consider when evalua...   \n",
       "3   Why does the author believe that intelligence ...   \n",
       "4   What aspects of the startup culture in the Val...   \n",
       "5   What are the potential consequences of not bei...   \n",
       "6   How did cheap bandwidth contribute to the grow...   \n",
       "7   What is the significance of a large market in ...   \n",
       "8   How can a transformative deal inject energy ba...   \n",
       "9   How does living a structured existence affect ...   \n",
       "10  How does perceived risk impact decision-making...   \n",
       "11  What is the author's perspective on the signif...   \n",
       "12  What task can showcase the skills of an outdat...   \n",
       "13  What are the consequences and trade-offs of th...   \n",
       "14  What's a simple and effective way to minimize ...   \n",
       "15  What is the purpose of using temporary subfold...   \n",
       "16  What factors determine the peak age of creativ...   \n",
       "17  What evidence beyond mainstream Internet busin...   \n",
       "\n",
       "                                             contexts  \\\n",
       "0   [that chance is immune from human intervention...   \n",
       "1   [you can reply to a lot of messages with “I’m ...   \n",
       "2   [How to hire the best people you've\\never work...   \n",
       "3   [Criteria 7rst\\nLots of people will tell you t...   \n",
       "4   [would have spent 10 or 20 or 30 years buildin...   \n",
       "5   [Here’s why you shouldn’t do that:\\nWhat are t...   \n",
       "6   [late 90’s — due to commodity hardware, open s...   \n",
       "7   [billion people online now. That is a very lar...   \n",
       "8   [cut some big deals, do some spinoWs, whatever...   \n",
       "9   [Arnold Schwarzenegger’s success as a movie st...   \n",
       "10  [sonal relationships within the group — so the...   \n",
       "11  [relation between raw intelligence, as measure...   \n",
       "12  [who, at their core, can’t program.\\nAnd it’s ...   \n",
       "13  [from that era that are doing well today versu...   \n",
       "14  [Answer the Xrst one when it rings, but never ...   \n",
       "15  [you can reply to a lot of messages with “I’m ...   \n",
       "16  [ods when productivity is highest, the peak ag...   \n",
       "17  [billion people online now. That is a very lar...   \n",
       "\n",
       "                                         ground_truth evolution_type  \\\n",
       "0                                                 nan         simple   \n",
       "1   The concept of commitment relates to organizin...         simple   \n",
       "2   There are many aspects to hiring great people,...         simple   \n",
       "3   The author believes that intelligence is overr...         simple   \n",
       "4   The culture of startups in the Valley contribu...         simple   \n",
       "5   The potential consequences of not being able t...         simple   \n",
       "6   Cheap bandwidth in the late 90's contributed t...         simple   \n",
       "7   A large market in the context of the internet ...         simple   \n",
       "8                                                 nan         simple   \n",
       "9   Living a structured existence can limit one's ...         simple   \n",
       "10  The desire to be liked in a startup with multi...  multi_context   \n",
       "11  The author believes that intelligence is signi...  multi_context   \n",
       "12                                                nan  multi_context   \n",
       "13  The consequences and trade-offs of the money r...  multi_context   \n",
       "14  One of the best and easiest ways to avoid dist...  multi_context   \n",
       "15  The purpose of using temporary subfolders in e...  multi_context   \n",
       "16  The peak age of creative impact is determined ...  multi_context   \n",
       "17                                                nan      reasoning   \n",
       "\n",
       "                                             metadata  episode_done  \n",
       "0   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "1   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "2   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "3   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "4   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "5   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "6   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "7   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "8   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "9   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "10  [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "11  [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "12  [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "13  [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "14  [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "15  [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "16  [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
       "17  [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "xAiXbVmLYSoC"
   },
   "outputs": [],
   "source": [
    "test_questions = test_df[\"question\"].values.tolist()\n",
    "test_groundtruths = test_df[\"ground_truth\"].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aE5rfMLfbqKH"
   },
   "source": [
    "Now we'll generate responses using our RAG pipeline using the questions we've generated - we'll also need to collect our retrieved contexts for each question.\n",
    "\n",
    "We'll do this in a simple loop to see exactly what's happening!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "9_AayvT1dAQN"
   },
   "outputs": [],
   "source": [
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "for question in test_questions:\n",
    "  response = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
    "  answers.append(response[\"response\"].content)\n",
    "  contexts.append([context.page_content for context in response[\"context\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opHaHmYDeBfC"
   },
   "source": [
    "Now we can wrap our information in a Hugging Face dataset for use in the Ragas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "fY48YZITeHy-"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "response_dataset = Dataset.from_dict({\n",
    "    \"question\" : test_questions,\n",
    "    \"answer\" : answers,\n",
    "    \"contexts\" : contexts,\n",
    "    \"ground_truth\" : test_groundtruths\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmeVvQaZeogE"
   },
   "source": [
    "Let's take a peek and see what that looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pOpydvc8eqNM",
    "outputId": "f924b59d-eb6b-4c1a-9d18-f545c4e2c724"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are the four major forms of chance and their reasons in human interactions and creative reactions?',\n",
       " 'answer': \"I don't know\",\n",
       " 'contexts': ['We can observe chance arriving in four major forms and for four\\ndiWerent reasons. The principles involved aWect everyone.\\nHere’s where it helps to be a neurologist writing on this topic:',\n",
       "  'unwilled, inadvertent, and unforseeable.\\nIndeed, chance plays several distinct roles when humans react cre-\\natively with one another and with their environment…',\n",
       "  'The four kinds of chance each have a diWerent kind of motor\\nexploratory activity and a diWerent kind of sensory receptivity.\\nThe [four] varieties of chance also involve distinctive personality',\n",
       "  'Luck and the entrepreneur: The four kinds of luck\\n163'],\n",
       " 'ground_truth': 'nan'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oM4fmAnsBmL2"
   },
   "source": [
    "# 🤝 Breakout Room Part #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbsFm5FievJI"
   },
   "source": [
    "## Task 1: Evaluating our Pipeline with Ragas\n",
    "\n",
    "Now that we have our response dataset - we can finally get into the \"meat\" of Ragas - evaluation!\n",
    "\n",
    "First, we'll import the desired metrics, then we can use them to evaluate our created dataset!\n",
    "\n",
    "Check out the specific metrics we'll be using in the Ragas documentation:\n",
    "\n",
    "- [Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html)\n",
    "- [Answer Relevancy](https://docs.ragas.io/en/stable/concepts/metrics/answer_relevance.html)\n",
    "- [Context Precision](https://docs.ragas.io/en/stable/concepts/metrics/context_precision.html)\n",
    "- [Context Recall](https://docs.ragas.io/en/stable/concepts/metrics/context_recall.html)\n",
    "- [Answer Correctness](https://docs.ragas.io/en/stable/concepts/metrics/answer_correctness.html)\n",
    "\n",
    "See the accompanied presentation for more in-depth explanations about each of the metrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "R2PXwyt8e5aW"
   },
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_correctness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_correctness,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kx-vlsx_hrtV"
   },
   "source": [
    "All that's left to do is call \"evaluate\" and away we go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "32514310070a426ea247c9f1bc66b630",
      "3e7520df71de40e0af5589b6aeb95171",
      "05390d20f1b445b5b02529ee7a99f6d6",
      "3380693903474d2585638f7e3458fcd6",
      "1d43002974f24e8a8b6961cddc04ce47",
      "97abe811c89c44dcacd7e39074d22546",
      "87a3d4b2ed5f4f1ca895c6a1981eb847",
      "589c2004f5504a239615dec8671785d0",
      "25d3337c457f4c748ed8bf78f5a27fe8",
      "31064d2adec14238a609d3f9791c64f3",
      "4f482b8ce7a54c1787394fb7d90391a0"
     ]
    },
    "id": "DhlcfJ4lgYVI",
    "outputId": "85fa2a99-7506-45d1-a674-ca9f55372264"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3e395be6744cc5a011bd7e1f7c15c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No statements were generated from the answer.\n",
      "Exception in thread Thread-11:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/ragas/executor.py\", line 95, in run\n",
      "    results = self.loop.run_until_complete(self._aresults())\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/asyncio/base_events.py\", line 654, in run_until_complete\n",
      "    return future.result()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/ragas/executor.py\", line 83, in _aresults\n",
      "    raise e\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/ragas/executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/asyncio/tasks.py\", line 615, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/ragas/executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/ragas/metrics/base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/ragas/metrics/base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
      "    results = await self.llm.generate(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/ragas/llms/base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 185, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/tenacity/__init__.py\", line 412, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 666, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1283, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/openai/_base_client.py\", line 1805, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/openai/_base_client.py\", line 1503, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/openai/_base_client.py\", line 1584, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/openai/_base_client.py\", line 1584, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/openai/_base_client.py\", line 1630, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mbrandao/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/openai/_base_client.py\", line 1599, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-mJHFwCKGpX06TnISgp1i95GO on tokens per min (TPM): Limit 60000, Used 59039, Requested 1481. Please try again in 520ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "ename": "ExceptionInRunner",
     "evalue": "The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mExceptionInRunner\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/ragas/evaluation.py:250\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, is_async, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evaluation_group_cm\u001b[38;5;241m.\u001b[39mended:\n\u001b[1;32m    248\u001b[0m         evaluation_rm\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m     result \u001b[38;5;241m=\u001b[39m Result(\n\u001b[1;32m    253\u001b[0m         scores\u001b[38;5;241m=\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_list(scores),\n\u001b[1;32m    254\u001b[0m         dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m    255\u001b[0m         binary_columns\u001b[38;5;241m=\u001b[39mbinary_metrics,\n\u001b[1;32m    256\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/aie3-w4d2/lib/python3.11/site-packages/ragas/evaluation.py:232\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, is_async, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[1;32m    230\u001b[0m results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mresults()\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;241m==\u001b[39m []:\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# convert results to dataset_like\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):\n",
      "\u001b[0;31mExceptionInRunner\u001b[0m: The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead."
     ]
    }
   ],
   "source": [
    "results = evaluate(response_dataset, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UqPArpSrgwDD",
    "outputId": "e8f3cb2f-8a38-47a5-f54d-ec80eaca8448"
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2nsGzj8DhP9E",
    "outputId": "a10d6394-0ab7-48bf-96c5-acfc5992622f"
   },
   "outputs": [],
   "source": [
    "results_df = results.to_pandas()\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWfiu_pLh3JL"
   },
   "source": [
    "## Task 2: Making Adjustments to our RAG Pipeline\n",
    "\n",
    "Now that we have established a baseline - we can see how any changes impact our pipeline's performance!\n",
    "\n",
    "Let's modify our retriever and see how that impacts our Ragas metrics!\n",
    "\n",
    "> NOTE: MultiQueryRetriever is expanded on [here](https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever) but for now, the implementation is not important to our lesson!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKIuM336isBL"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import MultiQueryRetriever\n",
    "\n",
    "advanced_retriever = MultiQueryRetriever.from_llm(retriever=retriever, llm=primary_qa_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82rcj3L-i_c8"
   },
   "source": [
    "We'll also re-create our RAG pipeline using the abstractions that come packaged with LangChain v0.1.0!\n",
    "\n",
    "First, let's create a chain to \"stuff\" our documents into our context!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EfdCgTw7jC4i"
   },
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(primary_qa_llm, retrieval_qa_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozYl5WdPnvLu"
   },
   "source": [
    "Next, we'll create the retrieval chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AK7wHVnn0U3"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(advanced_retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cmKORMfMoCjL"
   },
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"Who is Taylor Swift fueding with?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ICMsUWbWoOpf",
    "outputId": "1c2a8c65-0da8-44ef-d6e4-79dcca738777"
   },
   "outputs": [],
   "source": [
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5s8ZGasYoVi6"
   },
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"Why are they fueding?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ADNCdW4hoYT8",
    "outputId": "40860a4e-75fb-486e-b1e6-34a7eb2c5295"
   },
   "outputs": [],
   "source": [
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxkU0HdpoaiE"
   },
   "source": [
    "Well, just from those responses this chain *feels* better - but lets see how it performs on our eval!\n",
    "\n",
    "Let's do the same process we did before to collect our pipeline's contexts and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kO8cWxn2oinT"
   },
   "outputs": [],
   "source": [
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "for question in test_questions:\n",
    "  response = retrieval_chain.invoke({\"input\" : question})\n",
    "  answers.append(response[\"answer\"])\n",
    "  contexts.append([context.page_content for context in response[\"context\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgagfhPUtM2j"
   },
   "source": [
    "Now we can convert this into a dataset, just like we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FcllGeSovP8"
   },
   "outputs": [],
   "source": [
    "response_dataset_advanced_retrieval = Dataset.from_dict({\n",
    "    \"question\" : test_questions,\n",
    "    \"answer\" : answers,\n",
    "    \"contexts\" : contexts,\n",
    "    \"ground_truth\" : test_groundtruths\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dELYabwktR2C"
   },
   "source": [
    "Let's evaluate on the same metrics we did for the first pipeline and see how it does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "831b4dab6ff94d239d2824d390e01308",
      "4cefefc6cf714a68924e1b8d5e59aba9",
      "fd7f5542a22d44388dda12ca19443a1f",
      "93bf9b194c04460abffa192c19bcf67b",
      "83985f58744a46cfbd001ce5957f3e4a",
      "5c2b92989d7448e9bf65306c4f2f7d93",
      "a34b3906cd514234a115c7bf6757ca9d",
      "a03fefb9fa5a40ff947dc4ccd3c80318",
      "40343486e3ea4e5fae55b5a528f139d8",
      "cee2268aa21643e6ad77117c67ec1600",
      "c79f28da88f44d69aa87905c089df333"
     ]
    },
    "id": "d7uHseWJo2TU",
    "outputId": "8facfba9-467a-4129-b381-ddbd0a952f28"
   },
   "outputs": [],
   "source": [
    "advanced_retrieval_results = evaluate(response_dataset_advanced_retrieval, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JsFd0uDd2n5E",
    "outputId": "0922ab45-c16a-48c4-c9c2-3647ff130391"
   },
   "outputs": [],
   "source": [
    "advanced_retrieval_results_df = advanced_retrieval_results.to_pandas()\n",
    "advanced_retrieval_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0hzqq5VtZ2a"
   },
   "source": [
    "## Task 3: Evaluating our Adjusted Pipeline Against Our Baseline\n",
    "\n",
    "Now we can compare our results and see what directional changes occured!\n",
    "\n",
    "Let's refresh with our initial metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_WWGRaF5qx3V",
    "outputId": "7924b9a5-1bfc-4d26-f1f7-75e10fb64857"
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFv_yAeotmFs"
   },
   "source": [
    "And see how our advanced retrieval modified our chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rpV11dxJo7xa",
    "outputId": "2ce6e4b7-f037-4fd4-ca0c-1d47ff5dc522"
   },
   "outputs": [],
   "source": [
    "advanced_retrieval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "62NYn3iAvTjM",
    "outputId": "732eec56-d4ef-4403-cc90-62d0c81c37cf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_original = pd.DataFrame(list(results.items()), columns=['Metric', 'Baseline'])\n",
    "df_comparison = pd.DataFrame(list(advanced_retrieval_results.items()), columns=['Metric', 'MultiQueryRetriever with Document Stuffing'])\n",
    "\n",
    "df_merged = pd.merge(df_original, df_comparison, on='Metric')\n",
    "\n",
    "df_merged['Delta'] = df_merged['MultiQueryRetriever with Document Stuffing'] - df_merged['Baseline']\n",
    "\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJKEOLNs5v0R"
   },
   "source": [
    "## Task 4: Testing OpenAI's Claim\n",
    "\n",
    "Now that we've seen how our retriever can impact the performance of our RAG pipeline - let's see how changing our embedding model impacts performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MM4KRhJYEL-h"
   },
   "source": [
    "####🏗️ Activity #2:\n",
    "\n",
    "Please provide markdown, or code comments, to explain which each of the following steps are doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gv_tv4w86bPb"
   },
   "outputs": [],
   "source": [
    "# Creates the object that access the OpenAI Embeddings API, selecting which embedding model to use\n",
    "# Here we're selecting a different embedding model to test how much better is it\n",
    "new_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-JPe1_Jx6Rnw"
   },
   "outputs": [],
   "source": [
    "# Initiates the Qdrant vector store and loads the documents, in memory\n",
    "vector_store = Qdrant.from_documents(\n",
    "    documents,\n",
    "    new_embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"PMarca Blogs - TE3 - MQR\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-HuozNf6muZ"
   },
   "outputs": [],
   "source": [
    "# Creates a LangChain retriever from the vector store\n",
    "new_retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6Tyc3ZY7Km2"
   },
   "outputs": [],
   "source": [
    "# Creates a MultiQueryRetriever that is able to fetch a more diverse set of documents from the vector store\n",
    "new_advanced_retriever = MultiQueryRetriever.from_llm(retriever=new_retriever, llm=primary_qa_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5QSJIhm7SKr"
   },
   "outputs": [],
   "source": [
    "# Run the retrieval chain that retrieves documents and then passes them on. (function from LangChain)\n",
    "new_retrieval_chain = create_retrieval_chain(new_advanced_retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBVjl1UK7fd7"
   },
   "outputs": [],
   "source": [
    "# For each of the test question in our test set, invoke the retrieval chain and get the response from the llm.\n",
    "# Append the answers and the contexts to a list\n",
    "\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "for question in test_questions:\n",
    "  response = new_retrieval_chain.invoke({\"input\" : question})\n",
    "  answers.append(response[\"answer\"])\n",
    "  contexts.append([context.page_content for context in response[\"context\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTBrs0zr7iyG"
   },
   "outputs": [],
   "source": [
    "# Convert the new questions/answers/contexts/ground truth into a dataset\n",
    "new_response_dataset_advanced_retrieval = Dataset.from_dict({\n",
    "    \"question\" : test_questions,\n",
    "    \"answer\" : answers,\n",
    "    \"contexts\" : contexts,\n",
    "    \"ground_truth\" : test_groundtruths\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "399f6ec046c34c26818a07c5efc6845a",
      "7f82e0a3c3684460b8dda773d283b535",
      "cfa01f60b62f4a88806d85cee5ac0fa6",
      "38988d3f6f5f4de3b3d4be7fec89c3c7",
      "87bd0ad74d4345dea4b409d64524f6e7",
      "8cb506949697432db061878397d196f1",
      "e7831a581d024e3ebb4026a89ceef127",
      "18701fc64eb44d26b8aa1ae0af64d09f",
      "a6581091161c489d877c2cfec432f6ae",
      "42dcc945d1624f69b63bed2fa52cc4fa",
      "5524289f1e594a5eac60ee29d9f4249c"
     ]
    },
    "id": "hG5h-D8n7sZp",
    "outputId": "6760b49b-4576-4bee-e61a-293df24e2bc3"
   },
   "outputs": [],
   "source": [
    "# Run the function to evaluate the new results\n",
    "new_advanced_retrieval_results = evaluate(new_response_dataset_advanced_retrieval, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1uHdcpsZ76kj",
    "outputId": "53a4f4ef-13ce-4a89-a06e-6c255ed3c025"
   },
   "outputs": [],
   "source": [
    "# Print the results\n",
    "new_advanced_retrieval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "s4TyaCUQ79Ke",
    "outputId": "c496266c-bf4a-4a3a-cd27-1d8abbf5bdb3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_baseline \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[38;5;28mlist\u001b[39m(results\u001b[38;5;241m.\u001b[39mitems()), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMetric\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mADA + Baseline\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m df_original \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[38;5;28mlist\u001b[39m(advanced_retrieval_results\u001b[38;5;241m.\u001b[39mitems()), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMetric\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mADA + MQR\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m df_comparison \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[38;5;28mlist\u001b[39m(new_advanced_retrieval_results\u001b[38;5;241m.\u001b[39mitems()), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMetric\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTE3 + MQR\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Format the results for visualization\n",
    "df_baseline = pd.DataFrame(list(results.items()), columns=['Metric', 'ADA + Baseline'])\n",
    "df_original = pd.DataFrame(list(advanced_retrieval_results.items()), columns=['Metric', 'ADA + MQR'])\n",
    "df_comparison = pd.DataFrame(list(new_advanced_retrieval_results.items()), columns=['Metric', 'TE3 + MQR'])\n",
    "\n",
    "df_merged = pd.merge(df_original, df_comparison, on='Metric')\n",
    "df_merged = pd.merge(df_baseline, df_merged, on=\"Metric\")\n",
    "\n",
    "df_merged['ADA + MQR -> TE3 + MQR'] = df_merged['TE3 + MQR'] - df_merged['ADA + MQR']\n",
    "df_merged['Baseline -> TE3 + MQR'] = df_merged['TE3 + MQR'] - df_merged['ADA + Baseline']\n",
    "\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRmkcMrxC4Me"
   },
   "source": [
    "####❓ Question #4:\n",
    "\n",
    "Do you think, in your opinion, `text-embedding-3-small` is significantly better than `ada`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOciJLABDBnA"
   },
   "source": [
    "## BONUS ACTIVITY: Using a Better Generator\n",
    "\n",
    "Now that we've seen how much more effective a better Retrieval pipeline is, let's look at what impact a better(?) Generator is!\n",
    "\n",
    "Adapt the above `TE3 + MQR` pipeline to use `GPT-4o` and compare the results below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MY8l2EksDH43"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "primary_qa_llm_gpt4o = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "gpt4o_advanced_retriever = MultiQueryRetriever.from_llm(retriever=new_retriever, llm=primary_qa_llm_gpt4o)\n",
    "gpt4o_retrieval_chain = create_retrieval_chain(gpt4o_advanced_retriever, document_chain)\n",
    "\n",
    "answers_gpt4o = []\n",
    "contexts_gpt4o = []\n",
    "\n",
    "for question in test_questions:\n",
    "  response = gpt4o_retrieval_chain.invoke({\"input\" : question})\n",
    "  answers_gpt4o.append(response[\"answer\"])\n",
    "  contexts_gpt4o.append([context.page_content for context in response[\"context\"]])\n",
    "\n",
    "gpt4o_response_dataset_advanced_retrieval = Dataset.from_dict({\n",
    "    \"question\" : test_questions,\n",
    "    \"answer\" : answers_gpt4o,\n",
    "    \"contexts\" : contexts_gpt4o,\n",
    "    \"ground_truth\" : test_groundtruths\n",
    "})    \n",
    "\n",
    "gpt4o_advanced_retrieval_results = evaluate(gpt4o_response_dataset_advanced_retrieval, metrics)\n",
    "\n",
    "print(new_advanced_retrieval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MY8l2EksDH43"
   },
   "outputs": [],
   "source": [
    "# Format the results for visualization\n",
    "df_baseline = pd.DataFrame(list(results.items()), columns=['Metric', 'ADA + Baseline'])\n",
    "df_original = pd.DataFrame(list(advanced_retrieval_results.items()), columns=['Metric', 'ADA + MQR'])\n",
    "df_comparison = pd.DataFrame(list(new_advanced_retrieval_results.items()), columns=['Metric', 'TE3 + MQR'])\n",
    "df_comparison2 = pd.DataFrame(list(gpt4o_advanced_retrieval_results.items()), columns=['Metric', 'TE3 + MQR (GPT-4o)'])\n",
    "\n",
    "df_merged = pd.merge(df_original, df_comparison, on='Metric')\n",
    "df_merged = pd.merge(df_baseline, df_merged, on=\"Metric\")\n",
    "df_merged = pd.merge(df_merged, df_comparison2, on=\"Metric\")\n",
    "\n",
    "df_merged['ADA + MQR -> TE3 + MQR'] = df_merged['TE3 + MQR'] - df_merged['ADA + MQR']\n",
    "df_merged['Baseline -> TE3 + MQR'] = df_merged['TE3 + MQR'] - df_merged['ADA + Baseline']\n",
    "df_merged['Baseline -> TE3 + MQR (GPT-4o)'] = df_merged['TE3 + MQR (GPT-4o)'] - df_merged['ADA + Baseline']\n",
    "\n",
    "df_merged"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05390d20f1b445b5b02529ee7a99f6d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_589c2004f5504a239615dec8671785d0",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_25d3337c457f4c748ed8bf78f5a27fe8",
      "value": 100
     }
    },
    "05ab48866b5d49df9567ce9cbda5ee2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_49c1ef316e404052a7c8528781db3f9a",
       "IPY_MODEL_2dcb3e2fdf164e35a27a79cfae65933a",
       "IPY_MODEL_202f4244384a4501bfc1ffa50af96a1f"
      ],
      "layout": "IPY_MODEL_d93698b0506743ff98fdb998cfb7080a"
     }
    },
    "1317f4e20e1c4574a360345b427c3e8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_92ef10fab64c4f40a93da3d31b572016",
      "placeholder": "​",
      "style": "IPY_MODEL_3b43c3f561e34d019007ac9a0125b28d",
      "value": " 1248/1248 [00:46&lt;00:00, 13.97it/s]"
     }
    },
    "18701fc64eb44d26b8aa1ae0af64d09f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "19acd28bfa2e4a7a83bc42faea5de770": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d43002974f24e8a8b6961cddc04ce47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1fb5a4b71deb406fa2f342c88b9e4e1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7e1d22c19aff4c768d643c249e425d00",
      "placeholder": "​",
      "style": "IPY_MODEL_3a498872a68049329b4d206629b9b3bf",
      "value": "embedding nodes: 100%"
     }
    },
    "202f4244384a4501bfc1ffa50af96a1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a10a7577a99b4683a1d59a09d88f93a1",
      "placeholder": "​",
      "style": "IPY_MODEL_444bc7dae1aa4e098b79655428599310",
      "value": " 20/20 [01:04&lt;00:00, 10.00s/it]"
     }
    },
    "25d3337c457f4c748ed8bf78f5a27fe8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2aa53858803d4ad39113009d86dd67fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "2dcb3e2fdf164e35a27a79cfae65933a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_60a663f8736a43bcb47ac6c5f37ec597",
      "max": 20,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e6edc46811064de2b74a6a477c4a44b7",
      "value": 20
     }
    },
    "31064d2adec14238a609d3f9791c64f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32514310070a426ea247c9f1bc66b630": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3e7520df71de40e0af5589b6aeb95171",
       "IPY_MODEL_05390d20f1b445b5b02529ee7a99f6d6",
       "IPY_MODEL_3380693903474d2585638f7e3458fcd6"
      ],
      "layout": "IPY_MODEL_1d43002974f24e8a8b6961cddc04ce47"
     }
    },
    "3380693903474d2585638f7e3458fcd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31064d2adec14238a609d3f9791c64f3",
      "placeholder": "​",
      "style": "IPY_MODEL_4f482b8ce7a54c1787394fb7d90391a0",
      "value": " 100/100 [00:33&lt;00:00,  1.55s/it]"
     }
    },
    "356b929fa8dc42538767c58dcce12217": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "37ec9b5c847749439d7c155ac3b1ec68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89a7c333d0b241169dc29ed998b2c9c4",
      "max": 1248,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_88c8557741734e59a6099bb5fa260f6e",
      "value": 1248
     }
    },
    "38988d3f6f5f4de3b3d4be7fec89c3c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_42dcc945d1624f69b63bed2fa52cc4fa",
      "placeholder": "​",
      "style": "IPY_MODEL_5524289f1e594a5eac60ee29d9f4249c",
      "value": " 100/100 [00:45&lt;00:00,  1.68s/it]"
     }
    },
    "399f6ec046c34c26818a07c5efc6845a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7f82e0a3c3684460b8dda773d283b535",
       "IPY_MODEL_cfa01f60b62f4a88806d85cee5ac0fa6",
       "IPY_MODEL_38988d3f6f5f4de3b3d4be7fec89c3c7"
      ],
      "layout": "IPY_MODEL_87bd0ad74d4345dea4b409d64524f6e7"
     }
    },
    "3a498872a68049329b4d206629b9b3bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3b43c3f561e34d019007ac9a0125b28d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3e7520df71de40e0af5589b6aeb95171": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_97abe811c89c44dcacd7e39074d22546",
      "placeholder": "​",
      "style": "IPY_MODEL_87a3d4b2ed5f4f1ca895c6a1981eb847",
      "value": "Evaluating: 100%"
     }
    },
    "40343486e3ea4e5fae55b5a528f139d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "42dcc945d1624f69b63bed2fa52cc4fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "444bc7dae1aa4e098b79655428599310": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "49c1ef316e404052a7c8528781db3f9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_19acd28bfa2e4a7a83bc42faea5de770",
      "placeholder": "​",
      "style": "IPY_MODEL_356b929fa8dc42538767c58dcce12217",
      "value": "Generating: 100%"
     }
    },
    "4cefefc6cf714a68924e1b8d5e59aba9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c2b92989d7448e9bf65306c4f2f7d93",
      "placeholder": "​",
      "style": "IPY_MODEL_a34b3906cd514234a115c7bf6757ca9d",
      "value": "Evaluating: 100%"
     }
    },
    "4f482b8ce7a54c1787394fb7d90391a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5524289f1e594a5eac60ee29d9f4249c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "589c2004f5504a239615dec8671785d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c2b92989d7448e9bf65306c4f2f7d93": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "60a663f8736a43bcb47ac6c5f37ec597": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e1d22c19aff4c768d643c249e425d00": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f82e0a3c3684460b8dda773d283b535": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8cb506949697432db061878397d196f1",
      "placeholder": "​",
      "style": "IPY_MODEL_e7831a581d024e3ebb4026a89ceef127",
      "value": "Evaluating: 100%"
     }
    },
    "831b4dab6ff94d239d2824d390e01308": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4cefefc6cf714a68924e1b8d5e59aba9",
       "IPY_MODEL_fd7f5542a22d44388dda12ca19443a1f",
       "IPY_MODEL_93bf9b194c04460abffa192c19bcf67b"
      ],
      "layout": "IPY_MODEL_83985f58744a46cfbd001ce5957f3e4a"
     }
    },
    "83985f58744a46cfbd001ce5957f3e4a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87a3d4b2ed5f4f1ca895c6a1981eb847": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "87bd0ad74d4345dea4b409d64524f6e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88c8557741734e59a6099bb5fa260f6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "89a7c333d0b241169dc29ed998b2c9c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8cb506949697432db061878397d196f1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92ef10fab64c4f40a93da3d31b572016": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93bf9b194c04460abffa192c19bcf67b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cee2268aa21643e6ad77117c67ec1600",
      "placeholder": "​",
      "style": "IPY_MODEL_c79f28da88f44d69aa87905c089df333",
      "value": " 100/100 [00:44&lt;00:00,  1.69s/it]"
     }
    },
    "97abe811c89c44dcacd7e39074d22546": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a03fefb9fa5a40ff947dc4ccd3c80318": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a10a7577a99b4683a1d59a09d88f93a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a34b3906cd514234a115c7bf6757ca9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6581091161c489d877c2cfec432f6ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c79f28da88f44d69aa87905c089df333": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cee2268aa21643e6ad77117c67ec1600": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cfa01f60b62f4a88806d85cee5ac0fa6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_18701fc64eb44d26b8aa1ae0af64d09f",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a6581091161c489d877c2cfec432f6ae",
      "value": 100
     }
    },
    "d93698b0506743ff98fdb998cfb7080a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6edc46811064de2b74a6a477c4a44b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e7831a581d024e3ebb4026a89ceef127": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f75fdd56268a4b83a7fb7e4a3b2cce82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1fb5a4b71deb406fa2f342c88b9e4e1d",
       "IPY_MODEL_37ec9b5c847749439d7c155ac3b1ec68",
       "IPY_MODEL_1317f4e20e1c4574a360345b427c3e8a"
      ],
      "layout": "IPY_MODEL_2aa53858803d4ad39113009d86dd67fc"
     }
    },
    "fd7f5542a22d44388dda12ca19443a1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a03fefb9fa5a40ff947dc4ccd3c80318",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_40343486e3ea4e5fae55b5a528f139d8",
      "value": 100
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
